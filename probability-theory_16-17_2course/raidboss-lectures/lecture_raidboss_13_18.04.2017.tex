\section{Лекция от 18.04.2017}

\subsection{Пуассоновский процесс}

Процесс Пуассона служит для моделирования различных реальных потоков: 
несчастных случаев, потока заряженных частиц из космоса, отказов 
оборудования и других. Так же возможно применение для анализа финансовых 
механизмов, таких как поток платежей (об одной модели страхования мы 
поговорим позже).

\begin{definition}
  Пусть $\{X_t, t \geq 0\}$ --- случайный процесс. Он имеет 
  \textit{независимые приращения}, если $\forall n \in \N$ и
  $\forall 0 \leq t_1 < \ldots < t_n$ величины

  \[
    X_{t_n} - X_{t_{n - 1}}, \ldots, X_{t_2} - X_{t_1}, X_{t_1}
  \]

  независимы в совокупности.
\end{definition}

\begin{definition}
  Случайный процесс $\{N_t, t \geq 0\}$ называется
  пуассоновским процессом интенсивности $\lambda > 0$,
  если выполнены 3 условия:

  \begin{itemize}
    \item[1.] $N_0 = 0$ почти наверное.
    \item[2.] Процесс $N_t$ имеет независимые приращения.
    \item[3.] $N_t - N_s \sim \mathrm{Pois}(\lambda(t - s)) \ 
    \forall \ t \geq s \geq 0$. 
  \end{itemize}
\end{definition}

Конечно, первый вопрос, который возникает --- а существует ли такой 
процесс. Ответ --- да, существует, явную конструкцию мы разберем чуть
позже.

Сначала поймём несколько простых свойств. 

\begin{itemize}
  \item $N_t = N_t - N_0 \sim \mathrm{Pois}(\lambda t)$;
  \item Траектория процесса целочисленная (так как $N_t \in \Z_+$);
  \item Не только $N_t$ имеет пуассоновское распределение, но и
  $N_t - N_s$, а значит $N_t \geq N_s$ и траектория неубывающая.
\end{itemize}

Последние два свойства уже дают представление о том, как выглядит
пуассоновский процесс.


\begin{center}
  \begin{tikzpicture}
  
  \draw[line width=0.01mm,->] (0, 0) -- (12, 0) node[anchor=north, below left] {$t$};
  \draw[line width=0.01mm,->] (0, 0) -- (0, 7) node[anchor=east] {$N_t$};

  \draw (2.25, 0) node[draw,circle,fill=none,minimum size=4pt,inner sep=2pt] (C) {};
  \draw (5.46, 1) node[draw,circle,fill=none,minimum size=4pt,inner sep=2pt] (C) {};
  \draw (7.8, 2) node[draw,circle,fill=none,minimum size=4pt,inner sep=2pt] (C) {};
  \draw (8.8, 4) node[draw,circle,fill=none,minimum size=4pt,inner sep=2pt] (C) {};
  \draw (11.7, 5) node[draw,circle,fill=none,minimum size=4pt,inner sep=2pt] (C) {};

  \draw[very thick, -] (0, 0) -- (2.25, 0);
  \draw[very thick, -] (2.25, 1) -- (5.46, 1);
  \draw[very thick, -] (5.46, 2) -- (7.8, 2);
  \draw[very thick, -] (7.8, 4) -- (8.8, 4);
  \draw[very thick, -] (11.7, 5) -- (8.8, 5);

  \node [left] at (0, 0) {$0$};
  \node [left] at (0, 1) {$1$};
  \node [left] at (0, 2) {$2$};
  \node [left] at (0, 3) {$3$};
  \node [left] at (0, 4) {$4$};
  \node [left] at (0, 5) {$5$};
  \end{tikzpicture}
\end{center}


Нечто подобное мы уже встречали. Очень похоже на траекторию
восстановления. Это неспроста. На самом деле пуассоновский 
процесс --- процесс восстановления экспоненциального распределения.

\begin{theorem}[Явная конструкция пуассоновского процесса]
  Пусть $\{\xi_n, n \in \N\}$ --- независимые 
  $\mathrm{Exp}(\lambda)$ случайные величины.
  $S_n = \xi_1 + \ldots + \xi_n$ и введем процесс восстановления:

  $X_t = \sup(n : S_n \leq t), t \geq 0$.
  Тогда $X_t$ --- пуассоновский процесс интенсивности $\lambda$.
\end{theorem}

\begin{proof}
  Рассмотрим вектор $(S_1, \ldots, S_n)$. Так как этот вектор 
  легко получается линейным преобразованием из вектора $(\xi_1, \ldots, \xi_n)$.
  Причём мы можем явно указать, на какую матрицу надо домножить.

  \[
    \begin{pmatrix}
      S_1 \cr 
      S_2 \cr 
      S_3 \cr 
      \vdots \cr 
      S_n
    \end{pmatrix}
    =
    \begin{pmatrix}
      1 & 0 & 0 & \cdots & 0 \cr 
      1 & 1 & 0 & \cdots & 0 \cr
      1 & 1 & 1 & \cdots & 0 \cr
      \vdots & \vdots & \vdots & \ddots & \vdots \cr 
      1 & 1 & 1 & \cdots & 1
    \end{pmatrix}
    \begin{pmatrix}
      \xi_1 \cr 
      \xi_2 \cr 
      \xi_3 \cr 
      \vdots \cr 
      \xi_n
    \end{pmatrix}
  \]

  Плотность вектора суммы тогда очень легко вычисляется. У матрицы
  определелитель единичный. Поэтому плотность $(S_1, \ldots, S_n)$ равна

  \[
    \rho_{S_1, \ldots, S_n}(x_1, \ldots, x_n) = 
    \rho_{\xi_1, \ldots, \xi_n}(x_1, x_2 - x_1, \ldots, x_n - x_{n - 1})
  \]

  Но случайные величины $\xi_1, \ldots, \xi_n$ независимы, а значит плотность
  последнего вектора разбивается в произведение плотностей.

  \begin{multline}
    = \lambda^n e^{-\lambda x_1} e^{-\lambda (x_2 - x_1)} \ldots e^{-\lambda(x_n - 
    x_{n - 1})} \I\{x_1 \geq 0, x_2 \geq x_1, \ldots, x_n \geq x_{n - 1}\} =\\=
    \lambda^n e^{-\lambda x_n} \I\{x_n \geq x_{n - 1} \geq \ldots \geq x_1 \geq 0\}
  \end{multline}

  Вот такая плотность. Она нам понадобится для вычисления конкретных значений
  процесса восстановления.

  Зафиксируем какие-то моменты времени $0 < t_1 < \ldots < t_n$, будем для
  удобства считать, что $t_0 = 0$. А также возьмём целые неотрицательные числа
  $k_1 \leq k_2 \leq \ldots \leq k_n$. Чтобы доказать, что процесс пуассоновский,
  надо доказать, что приращения независимы и они имеют пуассоновское распределение.

  Рассмотрим вероятность:

  \[
    \Pr{X_{t_n} - X_{t_{n - 1}} = k_n - k_{n - 1}, \ldots, X_{t_2} - X_{t_1} = 
    k_2 - k_1, X_{t_1} = k_1}
  \]
  
  Надо доказать, что она разбивается на
  произведение вероятностей и ещё, что $X_{t_j} - X_{t_{j - 1}} \sim
  \mathrm{Pois}(\lambda(t_j - t_{j - 1}))$, то мы докажем, что хотели изначально.

  Чтобы оценить такую вероятность, давайте найдём зависимость от $X_i$ и $k_i$ с
  $S_i$.

  Ясно, что первые суммы $S_1, \ldots, S_{k_1} \in (0, t_1]$, после этого
  мы должны выйти из этого отрезка, то есть $S_{k_1 + 1}, \ldots, S_{k_2} \in
  (t_1, t_2]$ и так далее вплоть до $S_{k_n + 1} > t_n$, то есть вероятность равна:

  \[
    =\Pr{S_1, \ldots, S_{k_1} \in (0, t_1], \ldots, S_{k_n + 1} > t_n}
  \]

  Эту вероятность мы уже легко можем посчитать из плотности для случайного вектора
  сумм, которую мы вывели.

  \[
    = \idotsint\limits_{\substack{(x_1, \ldots, x_{k_n + 1}) \in \R^{k_n + 1}\\
    x_1, \ldots, x_{k_1} \in (0, t_1] \\ x_{k_1 + 1}, \ldots, x_{k_2} \in 
    (t_1, t_2] \\\ldots \\x_{k_n + 1} > t_n}}
    \lambda^{k_n + 1} e^{-\lambda x_{k_n + 1}} \I\{0 < x_1 \ldots < x_{k_n + 1}\}
    \,dx_1\,dx_2\,\ldots\,dx_{k_n + 1}
  \]

  На самом деле такой интеграл посчитать очень просто. У нас в функции интегрирования
  только одна переменная фигурирует и остаётся какой-то индикатор. Сведем
  к повторному интегралу:

  \[
    = \left(\int\limits_{t_n}^{+\infty} \lambda e^{-\lambda x_{k_n + 1}}\,
    dx_{k_n + 1}\right)
    \lambda^{k_n} \prod\limits_{j = 1}^n \ \idotsint\limits_{x_{k_{j - 1} + 1, \ldots
    x_{k_j} \in (t_{j - 1}, t_j]}} \I\{x_{k_{j - 1} + 1} < \ldots < x_{k_j}\}
    \,dx_{k_{j - 1} + 1}\,\ldots\,dx_{k_j}
  \]

  Мы вправе написать такое произведение интегралов, так как переменные разбиваются
  по группам и в каждой следующей группе значение больше, чем в предыдущей.

  Если бы в интеграле не стоял индикатор, то это был бы просто объем параллелепипеда.
  А у нас переменные все упорядочены. Значит перед нами объем симплекса, а как
  мы знаем, он в факториал раз меньше по размерности, чем объем параллелепипеда.

  Первый интеграл считается совсем просто --- он равен $e^{-\lambda t_n}$, поэтому
  получаем, что

  \[
    = e^{-\lambda t_n} \lambda^{k_n} \prod_{j = 1}^n \frac{(t_j - t_{j - 1})^{k_j 
    - k_{j - 1}}}{(k_j - k_{j - 1})!} =
    \prod_{j = 1}^n \frac{(\lambda(t_j - t_{j - 1}))^{k_j 
    - k_{j - 1}}}{(k_j - k_{j - 1})!} e^{-\lambda (t_j - t_{j - 1})}
  \]

  Последнее равенство проверяется непосредственно.

  А это как раз и есть то самое пуассоновское распределение, которое нам
  нужно. Более подробно: все случайные величины $X_{t_j} - X_{t_{j - 1}}$ независимы
  и имеют пуассоновское распределение, так как $\Pr{X_{t_j} - X_{t_{j - 1}} = k} =
  \frac{(\lambda (t_j - t_{j - 1}))^k}{k!} e^{-\lambda (t_j - t_{j - 1})}$.
\end{proof}

Выведем некоторые следствия из этой теоремы.

\begin{lemma}[Свойства пуассоновского процесса]
  \item[1.] С вероятностью один, все скачки пуассоновского процесса являются 
  единичными.
  \item[2.] Момент $n$-го скачка пуассоновского процесса (обозначим его за 
  $Y_n$) имеет гамма распределение с плотностью $p(x) =
  \frac{\lambda^n x^{n - 1}}{(n - 1)!}e^{-\lambda x} \I\{x > 0\}$.
  \item[3.] Случайные величины $Y_n - Y_{n - 1}$ имеют $\mathrm{Exp}(\lambda)$ 
  распределение и независимы.
\end{lemma}

\begin{proof}
  1) Пусть у нас есть $\{N_t, t \geq 0\}$ --- пуассоновский процесс, а 
  $\{X_t, t \geq 0\}$ --- явная конструкция.

  Тогда 
  \[
    \Pr{\text{у $N_t \ \exists$ скачок размер  $\geq 2$}} =
    \Pr{\text{у $X_t \ \exists$ скачок размер  $\geq 2$}} 
  \]

  Так как у нас процесс восстановления, скачок размера хотя бы два может возникнуть,
  когда какие-то два $S_n$ и $S_{n + 1}$ склеились в одно:

  \[
    = \Pr{\exists n: S_n = S_{n + 1}} = \Pr{\exists n: \xi_{n + 1} = 0} = 0
  \]

  Последнее равенство следует из того, что экспоненциальная величина принимает
  значение ноль с вероятностью ноль.

  2) Поймём, что $Y_n \eqdist S_n$, так как мы только что доказали, что скачок
  у $X_t$ с номером $n$ происходит в момент времени $S_n$, а
  $S_n = \xi_1 + \ldots + \xi_n$, где $\xi_i \sim \mathrm{Exp}(\lambda)$, но
  хорошо известно, что сумма экспоненциальных величин есть гамма распределение
  с плотностью, которую мы указали в лемме.

  3) Аналогично получаем, что $Y_n - Y_{n - 1} \eqdist S_n - S_{n - 1} = \xi_{n} \sim
  \mathrm{Exp}(\lambda)$.
\end{proof}

\subsection{Оценка вероятности разорения в модели страхования Крамера-Лундберга}

Вспомним саму модель.

Пусть $\{N_t, t \geq 0\}$ --- пуассоновский процесс интенсивности $\lambda > 0$,
а $\{\eta_n, n \in \N\}$ --- независимые одинаково распределенные неотрицательные
случайные величины, которые ещё и независимы с пуассоновским процессом. Тогда
модель определяется, как

\[
  X_t = y_0 + ct - \sum\limits_{k = 1}^{N_t} \eta_k, \text{ где $y_0, c > 0$}
\]

То есть у нас есть какой-то стартовый капитал. Деньги поступают к нам линейно.
Пуассоновский процесс считает, сколько трат нам надо было провести к моменту
времени $t$.

Больше всего нас интересует, когда компания разорится, то есть, если 
$\tau = \inf\{t, X_t < 0\}$, тогда нас интересует $\Pr{\tau < +\infty} \leq ?$.

Нам нужны будут некоторые предположения по поводу этой модели.

\begin{itemize}
  \item[1.] $\E{e^{v\eta_1}} < +\infty \ \forall v > 0$. Это условие согласуется
  на практике, так как можно считать, что $\eta_1$ ограничено;
  \item[2.] $c - a\lambda > 0$, где $a = \E{\eta_1} > 0$. Почему это условие 
  возникает? Давайте оценим $\E{X_t} = y_0 + ct - \E{\sum\limits_{k = 1}^{N_t} 
  \eta_k} = y_0 + ct - a\lambda t$. Последнее равенство предлагаем читателю проделать
  самостоятельно. И поэтому логично предполагать, что среднее у нас растёт, иначе
  мы точно разоримся.
\end{itemize}

Обозначим $\psi(v) = \E{e^{v\eta_1}}$. Введем процесс $Z_t = e^{-vX_t - g(v)t}$,
где $g(v) = \lambda(\psi(v) - 1) - vc$. В будущем мы убедимся, что $Z_t$ --- 
мартингал, но для этого докажем следующее утверждение.

\begin{lemma}
  $\E{e^{-v(X_t - X_s)}} = e^{g(v)(t - s)}, t \geq s$.
\end{lemma}

\begin{proof}
  Вместо того, чтобы сразу считать матожидание, посчитаем условное матожидание.
  \begin{multline}
    \E{e^{-v(X_t - X_s)} \given N_t = m, N_s = k} = 
    \E{e^{-v\left(c(t - s)) - \sum\limits_{j = N_s + 1}^{N_t} \eta_j\right)} \given
    N_t = m, N_s = k} =\\= \E{e^{-v\left(c(t - s)) - \sum\limits_{j = k + 1}^{m} 
    \eta_j\right)} \given N_t = m, N_s = k}
  \end{multline}

  У нас получаются независимые случайные величины в условии и самой величины,
  поэтому условие можно стереть.

  \[
    = \E{e^{-v\left(c(t - s)) - \sum\limits_{j = k + 1}^{m} 
    \eta_j\right)}}
  \]

  Первое это у нас константа, мы её и выносим, а также воспользуемся независимостью
  случайных величин.

  \[
    = e^{-vc(t - s)}\E{e^{v\sum\limits_{j = k + 1}^{m} 
    \eta_j}} = e^{-vc(t - s)} \prod_{j = k + 1}^m \E{e^{v\eta_j}}
  \]

  Множителей в произведении равно $m - k$, поэтому это всё равно:

  \[
    e^{-vc(t - s)} (\psi(v))^{m - k}
  \]

  Откуда мы получаем, что

  \[
    \E{e^{-v(X_t - X_s)}} = \E{\E{e^{-v(X_t - X_s)} \given N_t, N_s}} =
    \E{e^{-vc(t - s)} \psi(v)^{N_t - N_s}}
  \]

  Последнее равенство следует из только что выведенного условного математического
  ожидания.

  \[
    = e^{-vc(t - s)}\E{\psi(v)^{N_t - N_s}}
  \]

  Но $N_t - N_s \sim \mathrm{Pois}(\lambda(t - s))$, откуда

  \[
    e^{-vc(t - s)} \sum\limits_{k = 0}^{+\infty} (\psi(v))^k \frac{(\lambda(t - 
    s))^k}{k!}  e^{-\lambda(t - s)} = e^{-vc(t- s) + \lambda(t - s)\psi(v) -
     \lambda(t - s)}
  \]

  Группируем, остаётся:

  \[
    = e^{(t - s)(\lambda(\psi(v) - 1) - vc)}
  \]

  Это как раз то, что нам нужно.
\end{proof}

Установим, что $Z_t$ --- мартингал относительно $\mathbb{F}^X$ --- естественная
фильтрация $X_t$.

\begin{proof}
  Легко проверить, что $X_t$ имеет независимые приращения, поэтому 

  \[
    \E{Z_t \given \F_s^X} = \E{e^{-vX_t - tg(v)} \given \F_s^X}
  \]

  Так как $t, g(v)$ --- константы, можно их вынести. Также разобьём
  $X_t = X_t - X_s + X_s$ и воспользуемся тем фактом, что $X_s$ измерима
  относительно $\F_s^X$, а $X_t - X_s$ независимо с $\F_s^X$:

  \[
    = e^{-t g(v)}\E{e^{-v(X_t - X_s) - vX_s} \given \F_s^X} =
    e^{-t g(v) -vX_s} \E{e^{-v(X_t - X_s)}}
  \]

  Но последнее мы уже считали в предыдущей лемме, поэтому это всё равно:

  \[
    e^{-t g(v) - vX_s + (t - s)g(v)} = e^{-vX_s - s g(v)} = Z_s.
  \]
\end{proof}

Теперь мы хотим применить аналог теоремы об остановке мартингала для непрерывного
случая. Но чтобы её даже сформулировать, потребуется определение опционального
момента.

\begin{definition}
  $\tau: \Omega \to \R_+ \cup \{+\infty\}$ называется опциональным моментом 
  относительно фильтрации $\mathbb{F} = \{\F_t, t \geq 0\}$, если для любого
  $t \geq 0$ следует, что $\{\tau < t\} \in \F_t$.
\end{definition}

\begin{lemma}
  Пусть $\{X_t, t \geq 0\}$ --- процесс с непрерывными справа траекториями,
  пусть $G$ --- открытое множество, обозначим за $\tau_G = \inf\{t, X_t \in G\}$.
  Тогда $\tau_G$ --- опциональный момент относительно естественной фильтрации
  $\mathbb{F}^X$
\end{lemma}

\begin{proof}
  Из определения имеем, что

  \[
    \{\tau_G < t\} = \bigcup\limits_{s < t} \{X_s \in G\}
  \]

  Но траектории у нас непрерывны справа, а $G$ --- открытое (то существует шарик
  там, для которого $X_s \in G$), поэтому можно заменить, рассматривая
  все $s \in \Q$

  \[
    = \bigcup\limits_{\substack{s < t,\\s \in \Q}} \{X_s \in G\}
  \]

  Каждый из элементов лежит в $\F_s^X \subset \F_t^X$, поэтому и всё объединение
  (так как оно счётно) лежит в $\F_t^X$.
\end{proof}

\begin{theorem}[Теорема об остановке для непрерывного времени]
  Пусть $X_t$ --- мартингал относительно фильтрации $\mathbb{F} = \{\F_t, t \geq 0\}$
  с непрерывными справа траекториями. Если $\tau \leq \sigma$ --- ограниченные
  опциональные моменты относительно $\mathbb{F}$, то $\E{X_\tau} = \E{X_\sigma}$.
\end{theorem}

Эту теорему мы оставим без доказательства.

Вернемся снова к модели страхования. А именно, чтобы применить теперь теорему
об остановке, надо понять, какова траектория $X_t$.

Они выглядят примерно таким же образом, как и в модели страхования Спарре-Андресена
(см. первую лекцию). И траектории будут именно непрерывно справа, так как ровно
в момент времени $S_k$ мы что-то вычитаем. А значит и траектории $Z_t$ тоже
непрерывны справа, так как это экспонента от непрерывной справа функции.

Теперь нужно понять, что $\tau$ --- опциональный момент. Действительно,
$\tau = \inf\{t : X_t < 0\} = \inf\{t: X_t \in (-\infty, 0)\}$. Заметим,
что $(-\infty, 0)$ --- открытое множество. Значит $\tau$ --- опциональный
момент относительно $\mathbb{F}^X$ --- естественной фильтрации процесса $X_t$.

Но есть проблема, что $\tau$ не является ограниченным моментом, поэтому будем
теорему об остановке применять к самому $\tau$, а к его аппроксимации.

Рассмотрим для любого $t > 0$ величину $\min(\tau, t)$ --- тоже опциональный момент.
Но сейчас он у нас ограниченный. Значит по теореме об остановке для мартингала
$Z_t$ имеем, что $e^{-vy_0} = \E{Z_0} = \E{Z_{\min(\tau, t)}} =
\E{e^{-vX_{\min(\tau, t)} - \min(\tau, t)g(v)}}$. 

Добавим индикатор события $\{\tau < t\}$, тогда матожидание уменьшается, а минимум
раскрывается однозначно.

\[
  \geq \E{e^{-vX_\tau - g(v)\tau} \I\{\tau \leq t\}}
\]

Заметим, что $X_\tau \leq 0$, так как $X_\tau$ имеет непрерывные справа траектории.
Раз так, то экспонента с $X_\tau$ всегда не меньше единицы, поэтому можно писать
неравенство дальше

\[
  \geq \E{e^{-g(v)\tau} \I\{\tau \leq t\}}
\] 

Теперь мы хотим оценить первое слагаемое. Оценим его просто инфинумом по всем
$\tau = s \leq t$, поэтому получаем в итоге оценку:

\[
  \geq \inf\limits_{s \leq t} e^{-s g(v)} \Pr{\tau \leq t}.
\]

Откуда получается следующая оценка на вероятность:

\[
  \Pr{\tau \leq t} \leq e^{-v y_0} \sup\limits_{s \leq t} e^{g(v) s}.
\]

Оптимально выбрать нужно такое $v$, что функция под супремумом не растёт. Более
того мы хотим предъявить такое $v > 0$, что $g(v) = 0$, что нам даст
оценку не зависящую от $t$ и тогда мы сможем посчитать вероятность, что
$\tau < +\infty$.

Рассмотрим $g(v) = \lambda (\psi(v) - 1) - cv$, где $\psi(v) = \E{e^{v\eta_1}}$.
Легко понять, что $g(0) = 0$. Рассмотрим производную:

\[
  g'(v) = \lambda \psi'(v) - c = \lambda \E{\eta_1e^{v\eta_1}} - c
\]

Что неограниченно (так как в матождании экспонента) растет при $\eta_1 \neq 0$
почти наверное.

\[
  g''(v) = \lambda\E{\eta_1^2 e^{v\eta_1}} > 0, \text{ если $\eta_1 \neq 0$
почти наверное.}
\]

Поэтому первая производная растёт именно в большую сторону, то есть станет 
положительной с какого-то момента.

Поэтому у нас сначала функция убывает (так как $g'(0) = \lambda a - c < 0$), потом
возрастает, значит она ещё раз пересечет нуль. Пусть $g(v_0) = 0, v_0 > 0$.
Поэтому мы доказали теорему (просто устремим $t \to +\infty$):

\begin{theorem}
  Пусть $v_0 > 0$ --- решение уравнения $\lambda(\psi(v) - 1) - vc = 0$, тогда
  $\Pr{\tau < +\infty} \leq e^{-v_0y_0}$
\end{theorem}

