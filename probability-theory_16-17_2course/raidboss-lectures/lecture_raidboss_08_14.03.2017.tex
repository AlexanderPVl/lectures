\section{Лекция от 07.03.2017}

\subsection{Малые подграфы в случайном графе}

Начнём получать какие-то уже более конкретные результаты именно в этой теме.
А именно рассмотрим задачу о нахождении фиксированного графа $G$ в случайном
графе $G(n, p)$. Нас интересуют, конечно, несколько вопросов:

\begin{itemize}
  \item $\Pr{G(n, p) \text{ содержит копию } G}$;
  \item Какова пороговая вероятность по асимптотике?
  \item Пусть $X_n(G)$ --- число копий $G$ в случайном графе $G(n, p)$. Конечно,
  хотелось бы знать распределение этой случайной величины.
\end{itemize}

Будем постепенно заниматься каждым из этих вопросов. Пороговая вероятность будет
зависеть от понятия плотности графа и чем граф плотнее, тем пороговая вероятность
асимптотически меньше.

\begin{definition}
  Пусть граф $G$ имеет $v$ вершин и $e$ ребер, тогда \textit{плотностью} графа $G$ называют
  число $\rho(G) = \dfrac{e}{v}$.
\end{definition}

Конечно, с таким определением порой сложно что-то делать из-за того, что мы плохо
знаем структуру. Оказывается, что если ввести немного другое определение, то
работать станет значительно проще.

\begin{definition}
  Введем понятие \textit{максимальной плотности} графа
  \[
    m(G) = \max\{\rho(H), H \subseteq G\}
  \]
  Будем говорить, что если $m(G) = \rho(G)$, то граф \textit{сбалансированный}. Если
  $\forall H \varsubsetneq G \implies \rho(H) < \rho(G)$, то граф $G$ \textit{строго сбалансированный}.
\end{definition}

В определении под подграфом понимается какое-то подмножество вершин с ребрами между
ними из $G$. Приведем примеры на данные определения.

\begin{example}
  Многое из примеров останутся без доказательства из-за их лёгкости и 
  относительной простоты, предлагаем проделать эти доказательства 
  самостоятельно.
  \begin{itemize}
    \item $K_n$ --- строго сбалансированный граф с $m(G) = \frac{n - 1}{2}$;
    \item Цикл длины $m$ --- строго сбалансированный граф с $m(G) = 1$;
    \item Дерево --- строго сбалансированный граф с $m(G) = \frac{n - 1}{n}$;
    \item $K_4$ с одним дополнительным ребром из вершины. Тогда плотность $K_4$
    будет больше, чем у всего графа и граф не сбалансированный.
    \item Дерево с одним добавленным ребром, но не цикл. Тогда такой граф будет
    сбалансированным, но не строго.
  \end{itemize}
\end{example}

\begin{definition}
  Будем обозначать $\mathrm{aut}(G)$ --- размер группы автоморфизмов графа $G$,
  то есть количество таких $\phi: V \to V$, что $(u, v) \in E(G) \iff (\phi(u), \phi(v)) \in E(G)$.
\end{definition}

\begin{itemize}
  \item Ясно, что у $K_n$ это $\mathrm{aut}(K_n) = n!$;
  \item У цикла $C_n$ размер группы автоморфизмов равен $2n$ из-за того, что можно
  всё переставлять по циклу и ещё поменять направление;
  \item У полного двудольного графа $K_{n, m}$ при $n \neq m \implies \mathrm{aut}(K_{n, m}) =
  n!m!$, так как при автоморфизме должна сохраняться степень вершины, а между
  собой в доле вершины могут меняться любым образом. При $n = m$ мы можем ещё поменять
  доли (причём сразу все, иначе получим, что две вершины из одной доли станут соседними),
  поэтому в этом случае $\mathrm{aut}(K_{n, n}) = 2(n!)^2$.
\end{itemize}

\subsection{Среднее и дисперсия числа копий}

Вспомним, что глобально мы хотим ответить на вопрос о вероятности вхождения и пороговой
вероятности. Будем пользоваться вероятностным методом для ответа на эти вопросы.
Понятно, что нам надо воспользоваться методом первого и второго момента --- найти
среднее и дисперсию. Этим и займёмся.

Начнём со среднего. Понятно, что надо выбрать $v$ упорядоченных вершин из $n$ так, чтобы $e$
нужных ребер точно вошли, но так мы посчитаем чего-то лишнего --- а именно 
$\mathrm{aut}(G)$ способов для каждой расстановки, в итоге получим:

\[
  \E{X_n(G)} = \binom{n}{v}\frac{v!}{\mathrm{aut}(G)}p^e \asymp n^vp^e
\]

Последнее равенство можно писать из-за того, что $G$ фиксированный и биномиальный
коэффициент, умноженный на $v!$
это асимптотически $n^v$.

Теперь займёмся дисперсией, но введем перед этим одно определение:

\begin{definition}
  Будем обозначать $\Phi(G) = \min\{\E{X_n(H)} \ \forall H \subseteq G\}$.
\end{definition}

Эта величина возникнет в оценке дисперсии из-за ковариации. И так случилось, что
именно этот множитель потом даст нам пороговую вероятность $n^{-\frac{1}{m(G)}}$.

\begin{lemma}
  $\D{X_n(G)} \asymp (1 - p)\dfrac{(\E{X_n(G)})^2}{\Phi(G)}$
\end{lemma}

\begin{proof}
  Пусть $G_1, \ldots, G_N$ --- все копии вхождения графа $G$ в $K_n$, а
  $\I_1, \ldots, \I_N$ --- индикаторы вхождения этих копий. Тогда распишем дисперсию:

  \[
    \D{X_n(G)} = \sum\limits_{i, j} \cov{(\mathbf{I}_i, \mathbf{I}_j)}
  \]

  Если у $G_i$ и $G_j$ нет общих ребер, тогда ковариация равна нулю из-за независимости.
  А если эти графы пересекаются, тогда они пересекаются по какому-то графу $H
  \subseteq G$. Поэтому эту сумму можно расписать так, пользуясь свойствами ковариации:

  \[
    \sum\limits_{\substack{H \subseteq G,\\e(H) > 0}} \ \ \sum\limits_{\substack{i, j,\\G_i \cap G_j \cong H}}
    \E{\mathbf{I}_i \mathbf{I}_j} - \E{\mathbf{I}_i}\E{\mathbf{I}_j}
  \]

  После этого ясно, что матожидание у произведения индикаторов равно $p^{2e
  - e(H)}$, так как должно быть $2e$ ребер, из которых $e(H)$ повторяются,
  поэтому перепишем эту сумму так:

  \[
    \sum\limits_{\substack{H \subseteq G,\\e(H) > 0}} 
    p^{2e - e(H)}\left(1 - p^{e(H)}\right)\sum\limits_{\substack{i, j,\\G_i \cap G_j \cong H}} 1
  \]

  Теперь надо посчитать это самое количество. Опять же --- 
  выберем $2v$ вершин, из которых $v(H)$ должны совпадать. Как и в прошлом 
  случае это можно оценить асимптотически, как $n^{2v - v(H)}$, так как 
  число автоморфизмов это какая-то константа, а биномиальный коэффициент на 
  $v!$ оценивается именно такой степенью, то есть всё это равно

  \[
    \asymp \sum\limits_{\substack{H \subseteq G,\\e(H) > 0}} 
    p^{2e - e(H)}\left(1 - p^{e(H)}\right) n^{2v - v(H)} \asymp
    (\E{X_n(G)})^2 (1 - p) \sum\limits_{\substack{H \subseteq G,\\e(H) > 0}} 
    \frac{1}{p^{e(H)}n^{v(H)}}
  \]

  Давайте поясним, почему мы заменили $(1 - p^{e(H)})$ на $1 - p$.
  Если мы поделим одно на другое, то останется сумма $e(H)$ слагаемых, не б\'{о}льших
  единицы, что можно считать константой, а с другой стороны просто оценивается
  единицей. Квадрат матожидания поставили корректно (см. оценку средней величины).

  Осталось оценить последнюю сумму. Она снизу оценивается максимальным слагаемым,
  сверху суммой какого-то \textbf{константного} количества максимальных слагаемых.

  А максимальное слагаемое как раз по асимптотике равно $\frac{1}{\Phi(G)}$.

  Поэтому получаем такой ответ:

  \[
    \asymp (1 - p)\dfrac{(\E{X_n(G)})^2}{\Phi(G)}
  \]
\end{proof}

Наконец-то перейдём к пороговой вероятности, благо дисперсию и среднее мы уже
вычислили. Ответ на вопрос, какая пороговая вероятность даёт следующая

\begin{theorem}
  $\hat{p}(n) = n^{-\frac{1}{m(G)}}$ --- пороговая вероятность для свойства вхождения
  $G$ в случайный граф $G(n, p)$.
\end{theorem}

\begin{proof}
  Пусть $p(n) = o\left(\hat{p}\right)$, тогда верна цепочка неравенств для 
  $H \subseteq G$, на котором $\rho(H) = m(G)$:

  \begin{multline}
    \Pr{G(n, p) \text{ содержит копию } G} \leq \\\leq
    \Pr{G(n, p) \text{ содержит копию } H} = \Pr{X_n(H) \geq 1} \leq \E{X_n(H)}
  \end{multline}

  Последнее неравенство верно из-за неравенства Маркова. А последнее матожидание
  равно асимптотически $n^{v(H)}p^{e(H)}$, поэтому верна следующая цепочка:

  \[
    \asymp n^{v(H)}p^{e(H)} \asymp (np^{\rho(H)})^{v(H)} \asymp (np^{m(G)})^{v(H)}
    \to 0.
  \]

  Для доказательства в другую сторону воспользуемся методом второго момента. Пусть
  $\hat{p} = o\left(p(n)\right)$, тогда оценим $\Phi(G)$:

  \[
    \Phi(G) \asymp \min\limits_{H \subseteq G}\{n^{v(H)} p^{e(H)}\} \asymp
    \min\limits_{H \subseteq G}\{(np^{\rho(H)})^{v(H)}\} \geq
    \min\limits_{H \subseteq G}\{(np^{m(G)})^{v(H)}\} \to +\infty
  \]

  Предпоследнее неравенство следует из того, что $p < 1$, а показатель меньше.
  Теперь оценим вероятность того, что в $G(n, p)$ нет копии $G$ и докажем, что
  оно стремится к нулю с помощью следующих неравенств и неравенства Чебышева:

  \begin{multline}
    \Pr{G(n) \text{ не содержит копии } G} =\\=
    \Pr{X_n(G) = 0} \leq \Pr{|X_n(G) - \E{X_n(G)}| \geq \E{X_n(G)}} \leq
    \frac{\D{X_n(G)}}{(\E{X_n(G)})^2} \asymp \frac{1 - p}{\Phi(G)}
  \end{multline}

  Слагаемое последнее в любом случае стремится к нулю, так как знаменатель стремится
  к бесконечности, а числитель не больше единицы.
\end{proof}

Получаем следующие результаты пороговых вероятностей для некоторых графов:

\begin{itemize}
  \item Если $G = K_m$, тогда пороговая вероятность равна $\hat{p} = n^{-\frac{2}{m - 1}}$;
  \item Если $G = C_m$, тогда $\hat{p} = \frac1n$. В данном случае получается очень 
  интересная вещь: пороговая вероятность никак не зависит от $m$, то есть при достаточно
  больших $n$ если есть цикл длины три, то и есть цикл длины четыре и так далее;
  \item Если $G$ --- дерево на $m$ вершинах, тогда пороговая вероятность равна 
  $n^{-\frac{m}{m - 1}}$.
\end{itemize}

\subsection{Число копий строго сбалансированных графов}

Пороговая вероятность это, конечно, хорошо, но как обычно интереснее углубиться
внутрь и понять, что происходит, когда $np^{m(G)} \to c > 0$. На самом деле в
данном случае имеет место достаточно красивый результат, но правда только для
строго сбалансированных графов.

\begin{theorem}[Б. Боллобаш]
  Пусть $G$ --- строго сбалансированный граф и $n p^{m(G)} \to c > 0$, тогда
  число копий распределено по пуассоновскому закону $\mathrm{Pois}(\lambda)$, где
  $\lambda = \dfrac{c^{v(G)}}{\mathrm{aut}(G)}$.
\end{theorem}

В частности это, например, означает, что если $G = K_3$, то $\lambda = \frac{c^3}{6}$,
то есть вероятность, что копия есть равна $1 - e^{-\frac{c^3}{6}}$.

Доказывать эту теорему мы будем через метод моментов. Но он ничего общего не имеет
с методом, который
используется на курсе математической статистики.

\begin{definition}
  Распределение $X$ однозначно определяется своими моментами, если из того,
  что $\E{X^n} = \E{Y^n} \ \forall n \in \N$ следует, что $X \eqdist Y$.
\end{definition}

А следующие две леммы мы оставим, как вы, наверное, уже догадались, без доказательства.

\begin{lemma}
  Если $\exists \ \epsilon > 0$, что $|\E{e^{tx}}| < \infty$ при всех $t \in 
  (-\epsilon, \epsilon)$, то $X$ однозначно определяется своими моментами.
\end{lemma}

Из этой леммы получается, что все <<хорошие>> распределения определяются своими
моментами --- $\mathcal{N}(a, \sigma^2), \mathrm{Exp}(\lambda), \mathrm{Pois}(\lambda),
\mathrm{U}[a, b], \Gamma$ и другие. Предлагаем проверить это читателю самостоятельно.

\begin{lemma}[Метод моментов]
  Пусть $X$ однозначно определяется своими моментами. Тогда если 
  $\E{X_n^k} \to \E{X^k} \ \forall k \in \N$, то $X_n \dto X$.
\end{lemma}

Получается, что чтобы доказывать какие-то предположения о хороших распределениях,
надо лишь считать моменты и смотреть на их предел.

\begin{proof}[Доказательство теоремы 18.]
  Будем доказывать, что факториальные моменты совпадают, благо обычные моменты
  и факториальные друг через друга выражаются. Почему мы так хотим сделать? 
  -- Факториальные моменты у распределения Пуассона очень просто устроены и равны
  $\lambda^k$.

  Пусть $\I_1, \ldots, \I_N$ --- индикаторы вхождения копий $G_1,\ldots,G_N$ в
  $G(n, p)$. Тогда 

  \[
    \E{X_n(G)(X_n(G) - 1)\ldots(X_n(G) - k + 1)} =
    \sum\limits_{i_1 \neq i_2 \neq \ldots \neq i_k} \E{\mathbf{I}_{i_1}
    \ldots\mathbf{I}_{i_k}}
  \]

  Действительно, мы выбираем сначала какой-то подграф, потом выбираем какой-то,
  который не совпадает с предыдущим и так далее. Разобьём эту сумму на две ---
  $\mathsf{E'}_k$ --- случаи, когда $G_{i_1},\ldots,G_{i_k}$ попарно не имеют
  общих вершин и $\mathsf{E''}_k$ --- все остальные случаи.

  И как ни странно первое слагаемое даст уже весь вклад в подсчёт этого момента.
  Надо сначала выбрать $v(G)$ вершин, потом ещё $v(G)$ из оставшихся и так далее,
  каждый раз выбирая нужные $e(G)$ ребер.
  И каждый вариант в каждом множителе мы посчитаем $\mathrm{aut}(G)$ раз, на что
  мы благополучно делим.

  \begin{multline}
    \mathsf{E'}_k = \binom{n}{v(G)}\frac{v(G)!}{\mathrm{aut}(G)}p^{e(G)}
    \binom{n - v(G)}{v(G)}\frac{v(G)!}{\mathrm{aut}(G)}p^{e(G)}\cdot\ldots\cdot
    \binom{n - (k - 1)v(G)}{v(G)}\frac{v(G)!}{\mathrm{aut}(G)}p^{e(G)} 
    \asymp\\\asymp
    \left(\frac{n^{v(G)}p^{e(G)}}{\mathrm{aut}(G)}\right)^k
  \end{multline}

  Вопрос, конечно вызывает последнее асимптотическое равенство, но так как 
  $n \to +\infty$, то все биномиальные коэффициенты с дополнительными
  факториалами как раз будут давать $n^{v(G)}$. Но так как граф строго 
  сбалансированный, то $n^{v(G)}p^{e(G)}= (np^{m(G)})^{v(G)} \to c^{v(G)}$, откуда

  \[
    \left(\frac{n^{v(G)}p^{e(G)}}{\mathrm{aut}(G)}\right)^k \to 
    \left(\frac{c^{v(G)}}{\mathrm{aut}(G)}\right)^k = \lambda^k
  \]

  Осталось второе слагаемое и как мы уже поняли, надо доказать, что оно
  стремится к нулю. Как это сделать? Мы обсудим это в следующей лекции.
\end{proof}
