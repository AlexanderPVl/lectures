\documentclass[a4paper, 12pt]{article}
%\input{header.sty}
%\usepackage{algorithm}
%\usepackage[noend]{algpseudocode}


\begin{document}
%\maketitle

\section{Лекция 3. Безусловная многомерная минимизация}

Рассмотрим следующую задачу минимизации функции: $$f(x) \rightarrow \min_{x \in \R^n}, f \in C^1, O(x) = \{f(x), \nabla f(x)\}$$

Напомним, что $O(x)$ - оракул, то есть совокупность величин, которые возможно вычислить в заданной точке $x$ (см. лекцию 2).

Из методов многомерной оптимизации можно выделить два вида: 

\begin{enumerate}
    \item Методы поиска по направлению (Line Search). 
    \item Методы доверительной области (Trust Region).
\end{enumerate}

\subsection{Методы поиска по направлению (Line Search)}

В методах поиска по направлению на каждом шаге значение рассматриваемой точки - кандидата на локальный минимум - изменяется по следующему правилу:
$$x_{k+1} = x_k + \alpha_{k}d_{k}$$
$d_{k} \in \R^n$ - \textit{вектор спуска}, то есть для функции $\phi(\alpha) &= f(x_k + \alpha d_k)$ выполняется условие: 
$$\phi'(0) &=\nabla f(x_k)^T d_k  < 0$$

$\alpha_{k} \in \R_+ $ - длина шага. Для нахождения $\alpha_{k}$ решается задача одномерной оптимизации:
$$\alpha_k = \argmin_\alpha f(x_k + \alpha d_k)$$

\subsection{Методы доверительной области (Trust Region)}

В методах доверительной области следующая точка изменяется по правилу $x_{k+1} = x_k + d_k$. Для нахождения $d_k$ решается система уравнений:
\begin{align*}
\begin{cases}
        f(x_k + d_k) \approx m_k(d_k) = f(x_k) + g_k^Td_k+\frac{1}{2}d_k^TB_kd_k \rightarrow \min_{d_k}\\
        ||d_k|| \leq \Delta_k
\end{cases}
\end{align*}

Здесь $\Delta_k$ - радиус доверительной области, $g_k = \nabla f(x_k)$, $B_k$ - гессиан или его аппроксимация.

Второе уравнение задает доверительную область (trust region). Заметим, что минимизация уже не глобальная, а на доверительной области.

Trust Region - более гибкий подход. При изменении радиуса доверительной области направление $d_k$ может измениться. При увеличении радиуса качество подбора направления возрастает. Однако при использовании такого подхода на каждом шаге нужно решать многомерную задачу оптимизации, в то время как Line search сводится к одномерной. 

%TODO: картика, иллюстрирующая изменение направления d_k от величины окрестности

На практике Trust Region используют при "дорогом" оракуле. В таком случае оракул запускается меньшее количество раз, на каждом шаге задача оптимизации считается более качественно. 

\subsection{Вычисление длины шага в линейном поиске}

Вернемся к линейному поиску и рассмотрим несколько методов вычисления длины шага $\alpha_k$. Введем функцию $\phi(x) = f(x_k + \alpha_k d_k)$, чтобы $d_k$ был вектором спуска должно выполняться условие $\phi'(0) < 0$.

\subsubsection*{Условие Армихо}

Условием Армихо называется выполнение неравенства:
$$\phi(\alpha_k) \leq \phi(0) + c_1\alpha_k\phi'(0), \quad c_1 \in (0,1) $$

\subsubsection*{Условия Вульфа}

Условия Вульфа являются дополнениями к условию Армихо и дополнительно требуют следующее:

Слабое условие Вульфа:
$$\phi'(\alpha_k) \geq c_2 \phi'(0) $$
Сильное условие Вульфа:
$$|\phi'(\alpha_k)| \leq c_2 |\phi'(0)| $$

Здесь $c_2 \in (c_1,1)$.

\begin{Statement}
Пусть $\phi \in C^1$ - непрерывно-дифференцируема и для любого $\alpha$ выполняется $\phi(\alpha) > -\infty$. Тогда для любых $c_1$ и $c_2$: $0 < c_1 \leq c_2 < 1$ найдется $\alpha_k$, удовлетворяющая условию Вульфа.
\end{Statement}

На практике берут $c_1 = 10^{-4}$ и $c_2 = 0.9$ и $0.1$ - для достаточно грубой и более точной одномерной оптимизации соответственно.

%TODO: картиночка с иллюстрацией условия Армихо, слабого и сильного Вульфа

Чтобы найти $\alpha_k$, удовлетворяющую условиям, используем метод, аналогичный методу Брента. Алгоритм можно разбить на 2 шага:

\begin{enumerate}
    \item Этап расширения: расширение интервала, до тех пор, пока не будут выполнены условия нахождения искомой точки в рассматриваемом интервале.
    \item Этап сужения: ищем точку внутри (например, с помощью метода золотого сечения) и уменьшаем интервал. 
\end{enumerate}

Для нахождения точки, удовлетворяющей условию Армихо, используют метод дробления шага:

\begin{algorithm}
\caption{Процедура backtracking}\label{alg:backtrack}
\begin{algorithmic}[1]
\State $\alpha \gets \alpha_0$
\Loop
    \If {$\phi(\alpha) \leq \phi(0) + \alpha c_1 \phi'(0)$} \Then
    \State\textbf{break}
    \EndIf
    \State $\alpha \gets \alpha \rho$, $\rho < 1$
\EndLoop
\end{algorithmic}
\end{algorithm}

Теперь построим метод, выбирающий вектор спуска $d_k$.

\subsection{Метод градиентного спуска}

Сформулируем задачу линейного поиска так:
$$x_{k+1} = x_k + \alpha_k d_k$$
\begin{align*}
\begin{cases}
        \nabla f(x_k)^Td_k \rightarrow \min_{d_k}\\
        ||d_k||^2 \leq 1
\end{cases}
\end{align*}

Выразим отсюда направление спуска:
$$d_k = -\frac{\nabla f(x_k)}{||\nabla f(x_k)||}$$

Полученное $d_k$ соответствует выбору направления на шаге градиентного спуска:
$$x_{k+1} = x_k - \alpha_k \nabla f(x_k)$$

Стратегии выбора длины шага $\alpha_k$:

\begin{enumerate}
    \item $\alpha_k = \frac{1}{L}$, где L - константа Липшица, $f \in C^{1,1}_L$
    \item  $\alpha_k$ удовлетворяет условиям Армихо-Вольфа для заданных $c_1, c_2$
\end{enumerate}

Рассмотрим более подробно первую стратегию. Если функция $f \in C^{1,1}_L$, то ее можно ограничить параболой.
$$f(y) \leq f(x) + \nabla f(x)^T (y - x) +\frac{L}{2} ||y - x||^2 = (*) $$

Попробуем оценить функцию глобально. Вновь воспрользуемся тем, что функция Липшицева, а значит, ее градиент ограничен. Пусть $y = x - \alpha \nabla f(x)$, тогда:
$$(*) = f(x) -\alpha ||\nabla f(x)||^2 + \frac{L \alpha ^2}{2} ||\nabla f(x)||^2 = f(x) - \alpha(1-\frac{L \alpha}{2})||\nabla f(x)||^2$$

Возьмем $\alpha$ такое, чтобы коэффициент при норме градиента был наибольшим: 
$$\alpha\left(1-\frac{L \alpha}{2}\right) \rightarrow \max_{\alpha}$$
Тогда $\alpha_{opt} = \frac{1}{L}$ и $\alpha_{opt}\left(1-\frac{L \alpha_{opt}}{2}\right)  = \frac{1}{2L}$

Значит, гарантированный прогресс на итерации работы алгоритма равен $\frac{1}{2L}||\nabla f(x)||^2$. 

%TODO: картиночка

$$f(x_{k+1}) \leq f(x_{k}) - \frac{1}{2L}||\nabla f(x_k)||^2 \leq f(x_{k-1}) - \frac{1}{2L}||\nabla f(x_{k-1})||^2 - \frac{1}{2L}||\nabla f(x_{k})||^2 \leq \dots$$
$$\dots \leq f(x_{0}) - \frac{1}{2L}\sum_{i = 0}^{k}||\nabla f(x_i)||^2$$
$$\sum_{i = 0}^{k}||\nabla f(x_i)||^2 \leq 2L(f(x_0) - f(x_{k+1})) \leq 2L(f(x_0) - f_{opt})$$

При $ k \rightarrow \infty$ ряд сходится, значит, $||\nabla f(x_{k-1})||^2 \rightarrow 0$

Подсчитаем скорость, с которой $\nabla f(x_{k-1})$ стремится к 0:

$$g_k = \min_{0\leq i\leq k}||\nabla f(x_{i})||^2$$

$$(k+1)g_k \leq \sum_{i = 0}^{k}||\nabla f(x_i)||^2 \leq 2L(f(x_0) - f_{opt})$$

\begin{Statement}
Если $ f \in C^{1,1}_L$ и $\forall x f(x) > -\infty $, то для градиентного спуска с $\alpha_k = \frac{1}{L}$ верно, что $g_k \leq \frac{c}{k+1}$, где c - некоторая константа.
\end{Statement}

Попробуем усилить предположение. Добавим требование к выпуклости функции. Пусть $ f \in C^{1,1}_L$ и $f$-сильно выпуклая с параметром $\mu$. По определению сильной выпуклости:
$$ f(y) \geq f(x) + \nabla f(x)^T (y - x) + \frac{\mu}{2}\| y - x \|^2 = g(y)$$
$$f_{opt} = \min_y f(y) \geq \min_y g(y) $$
$$\nabla_y g(y) = \nabla f(x) + \mu(y - x) = 0$$
$$y_{opt} = x - \frac{1}{\mu}\nablaf(x), \quad g(y_{opt}) = f(x) - \frac{1}{2\mu} \| \nabla f(x) \|^2$$

$$f_{opt} \geq f(x) - \frac{1}{2\mu} \| \nabla f(x) \|^2 \quad \rightarrow \quad \| \nabla f(x) \|^2 \geq 2\mu(f(x) - f_{opt}) $$

Объединим полученные неравенства:
$$f(x_k) - f_{opt} \leq f(x_k) - f_{opt} - \frac{1}{2L} \| \nabla f(x) \|^2 \leq f(x_k) - f_{opt} - \frac{\mu}{L} (f(x) - f_{opt}) = (1 - \frac{\mu}{L})(f(x_0) - f_{opt})$$

\begin{Statement}
Если $ f \in C^{1,1}_L$, и $f$ - сильно выпуслая с параметром $\mu$, то для градиентного спуска с $\alpha_k = \frac{1}{L}$ верно, что $f(x_k) - f{opt} \leq (1 - \frac{\mu}{L}(f(x_0 - f_{opt})$.
\end{Statement}

\begin{table}[h]
    \centering
    \begin{tabular}{|c|c|c|} 
        \hline
        Функция & Невязка $r_k$ & Скорость сходимости\\
        \hline
        \hline
        $f \in C^{1,1}_L $ & $\min_{0\leq i \leq k}\|\nabla f(x_i)\|^2$& $O(\frac{1}{k})$\\
        \hline
        $f \in C^{1,1}_L$, $f$ - выпуклая & $f(x_k) - f_{opt}$& $O(\frac{1}{k})$\\
        \hline
        $f \in C^{1,1}_L$, $f$ - $\mu$-сильно выпуклая & $f(x_k) - f_{opt}$& $O(c^k),\: c = 1 - \frac{\mu}{L}$\\
        \hline
    \end{tabular}
    \caption{Скорость сходимости градиентного спуска}
\end{table}


\end{document}