\documentclass[a4paper, 12pt]{article}
%\input{header.sty}
%\usepackage{algorithm}
%\usepackage[noend]{algpseudocode}

\begin{document}
%\maketitle
\section{Лекция 5}

Вспомним, что в методе Ньютона направление $d_k = -B_k^{-1}\nabla f(x_k)$, но матрицу $B_k$ мы численно не обращаем, а решаем СЛАУ $B_k d_k = -\nabla f(x_k)$. При этом $B_k = B_k^T > 0$ --- симметричная положительно определенная матрица. Мы выведем метод решения СЛАУ $Ax = b$ с положительно определенной симметричной матрицей $A$, который требует лишь уметь вычислять произведение матрицы $A$ на произвольный вектор $x$. Преимущество такого подхода заключается в том, что нам не придется вычислять матрицу $B_k$, а лишь уметь умножать её на произвольный вектор. О том, как этого можно добиться, будет рассказано в следующей лекции. А пока попытаемся разобраться в \textit{методе сопряженных градиентов}.

\subsection{Сопряженный набор векторов}
Заметим, что решение системы $Ax = b$, $A = A^T > 0$ эквивалентно минимизации
$$f(x) = \frac{1}{2} x^T A x - b^T x \to \min_x$$
Действительно, необходимое условие минимума функции $\nabla f(x) = Ax - b = 0$, и это решение и впрямь доставляет минимум в силу выпуклости функции $f(x)$ (вспомним, что матрица $A \in \mathbb{S}^n_{++}$).

Эту задачу можно решать методом спуска, т.е. итеративно строить последовательность решений $x_{k+1} = x_k + \alpha_k d_k$. Заметим, что если мы будем знать направление $d_k$, мы можем вычислить оптимальную величину шага $\alpha_k$, которое соответствует наискорейшему спуску. Введём обозначение $g_k = \nabla f(x_k) = Ax_k - b$. Тогда
\begin{align*}
\frac{d}{d\alpha} f(x_k + \alpha d_k) &= \nabla f(x_k + \alpha d_k) ^T d_k = [A(x_k + \alpha d_k) - b]^T d_k \\ &= (Ax_k - b)^T d_k + \alpha d_k^T A d_k = g_k^T d_k + \alpha d_k^T A d_k = 0
\end{align*}
откуда
$$\boxed{\alpha_k = -\frac{g_k^T d_k}{d_k^T A d_k}}$$

\begin{Def}
Набор векторов $\{d_i\}_{i=0}^{n-1}$ ($d_i \in \mathbb{R}^n$) называется сопряжённым относительно матрицы $A$, если $d_i^TAd_j = 0$ для любых $i \neq j$.
\end{Def}
\begin{Comment}
Заметим, что существует некоторый базис, в котром матрица Грама будет равна $A$ (т.к. она симметрична и положительно определена). В этом пространстве $\langle d_i, d_j \rangle_A = d_i^T A d_j = 0$ для любых $i \neq j$, т.е. в этом пространстве $\{d_i\}_{i=0}^{n-1}$ --- ортогональный набор векторов.
\end{Comment}

\begin{Statement}
Если $\{d_i\}_{i=0}^{n-1}$ --- сопряженный относительно $A$ набор веткоров, то эти векторы линейно независимы.
\end{Statement}
\begin{proof}
Приравняем линейную комбинацию $\{d_i\}_{i=0}^{n-1}$ к нулю: $\alpha_1 d_1 + \dots + \alpha_n d_n = 0$. Умножим обе части слева на $d_j^T A$. В силу того, что для $i \neq j$ имеем $d_j^T A d_i = 0$, получим, что
$$\alpha_j d_j^T A d_j = 0$$
Но матрица $A$ положительно определена, что означает, что $d_j^T A d_j > 0$, а значит $\alpha_j = 0$. Заключаем, что $\alpha_1 = \dots = \alpha_n = 0$, и векторы $\{d_i\}_{i=0}^{n-1}$ линейно независимы.
\end{proof}

Следствием этого является то, что система $\{d_i\}_{i=0}^{n-1}$ является базисом: она линейно независима и состоит из $n$ векторов. Пусть $x_{\mathrm{opt}}$ --- точка локального минимума, т.е. $Ax_{\mathrm{opt}} = b$. В силу того, что это базис, по нему можно разложить вектор $x_{\mathrm{opt}} - x_0$:
$$x_{\mathrm{opt}} - x_0 = \sum_{i=0}^{n-1} \alpha_i d_i$$
Опять умножим обе части слева на $d_j^TA$ и воспользуемся тем, что $d_j^TAd_i = 0$ при $i \neq j$. Получим, что 
$$d_j^T A(x_{\mathrm{opt}} - x_0) = \alpha_jd_j^TAd_j \Rightarrow \alpha_j = \frac{d_j^T A(x_{\mathrm{opt}} - x_0)}{d_j^TAd_j} = \frac{d_j^T (b - Ax_0)}{d_j^TAd_j} = -\frac{g_0^T d_j}{d_j^TAd_j}$$

Видим, что полученное выражение очень похоже на направление наискорейшего спуска, соответствующее направлению $d_j$. На самом деле, эти значения равны, о чём говорит нам следующее

\begin{Statement}
$g_k^Td_k = g_0^Td_k$.
\end{Statement}
\begin{proof} Действительно, вспомним, что $x_k$ --- это точка после $k$-го спуска:
\begin{align}
    g_k^T d_k &= (Ax_k - b)^T d_k = \left(A \left(x_0 + \sum_{i=0}^{k - 1} \alpha_i d_i \right) - b \right)^T d_k \\ &= (Ax_0 - b)^T d_k + \underbrace{\sum_{i=0}^{k-1}\alpha_i d_i^T A d_k}_{=0} \\ &= g_0^T d_k
\end{align} Сумма равна нулю в силу того, что индекс в сумме пробегает значения от 0 до $k-1$, а значит не равен $k$.
\end{proof}

Следующее утверждение говорит нам о том, что $g_k$ ортогонален предыдущим направлениям спуска $d_j$.
\begin{Statement}
$g_k^Td_j = 0$ при $j < k$.
\end{Statement}
\begin{proof} Вспомним, что $g_k = Ax_k - b$, а значит
\begin{align}
    g_k^T d_j &= (Ax_k - b)^T d_j = \left(A\left(x_0 + \sum_{i=0}^{k-1} \alpha_i d_i\right) - b\right)^T d_j \\ &= (Ax_0 - b)^T d_j + \sum_{i=0}^{k-1} \alpha_i d_i^T A d_j = g_0^Td_j + \alpha_j d_j^T A d_j \\ &= \left\{ \alpha_j = -\frac{g_0^Td_j}{d_j^TAd_j}\right\} = 0
\end{align}
\end{proof}

Заметим, что так как $g_k$ ортогонален $d_j$ при $0 \leq j \leq k-1$, он ортогонален и линейной оболочке $\langle d_0, d_1, \dots, d_{k - 1} \rangle$. При этом на каждой итерации мы совершаем шаг наискорейшего спуска. Это означает, что $x_k$ является оптимумом в подпространстве $x_k + \langle d_0, d_1, \dots, d_{k - 1} \rangle = x_0 + \sum\limits_{i=0}^{k-1} \alpha_i d_i + \langle d_0, d_1, \dots, d_{k - 1} \rangle = x_0 + \langle d_0, d_1, \dots, d_{k - 1} \rangle$. Выведенное свойство называется \textit{частичной оптимальностью}. По сути, оно означает, что после $k$-го шага мы добиваемся оптимальности в $x_0 + \langle d_0, d_1, \dots, d_{k - 1} \rangle$, и, постепенно расширяя линейную оболочку, за $n$ шагов добьёмся оптимальности в $x_0 + \langle d_0, d_1, \dots, d_{n - 1} \rangle = \mathbb{R}^n$.

Рассмотрим матрицу перехода $C$ к базису $d_0, \dots, d_{n-1}$ (по столбцам матрицы $C$ записаны координаты базисных векторов $d_0, \dots, d_{n-1}$ в текущем базисе). Тогда если за $\hat{x}$ обозначит координаты вектора $x$ в новом базисе, $x = C\hat{x}$. Рассмотрим функцию $\hat{f}(\hat{x}) = f(C\hat{x})$:
\begin{align}
    \hat{f}(\hat{x}) &= f(C\hat{x}) = \frac{1}{2}(C\hat{x})^T A (C\hat{x}) - (C\hat{x})^T b = \frac{1}{2} \hat{x}^TC^TAC\hat{x} - \hat{x}^TC^Tb \\ &= \left\{C^TAC=\diag(\lambda_0, \dots, \lambda_{n-1})\right\} =  \frac{1}{2} \sum_{i=0}^{n-1} \left[ \lambda_i\hat{x}_i^2 - 2\hat{x}_i (C^Tb)_i \right] \to \min
\end{align}
Здесь мы воспользовались тем, что матрица квадратичной формы $A$ в базисе $d_0, \dots, d_{n-1}$ будет иметь диагональный вид. Действительно, $d_i^TAd_j = 0$ при $i \neq j$, а $d_i^TAd_i$ мы обозначили за $\lambda_i$. Заметим теперь, что на последнем шаге преобразований мы привели функцию к сепарабельному виду, т.е. каждое слагаемое можно оптимизировать отдельно, так как все они независимы. Это мы и делаем, спускаясь по направлениям $d_0, \dots, d_{n-1}$: сначала, делая шаг наискорейшего спуска по направлению $d_0$, мы находим координату $\hat{x}_0$, затем --- $\hat{x}_1$ и т.д., пока не отыщем весь вектор.

Итак, имея набор $\{d_i\}_0^{n-1}$, мы за $n$ итераций сможем найти точное решение системы уравнений $Ax = b$. Осталось только понять, как же предъявить такой набор. Первое, что приходит в голову, --- взять произвольный набор линейно независимых векторов (например, стандартный базис) и выполнить ортогонализацию Грама-Шмидта со скалярным произведением $\langle x, y \rangle = x^TAy$. В этом случае наши затраты по памяти составят $O(n^2)$, что несомненно много: можно лучше.

\subsection{Метод сопряженных градиентов}
Оказывается, что набор, который строится по следующим правилам:
\begin{align}\label{cgrad}
    d_0 &= -g_0 \\
    d_{k+1} &= -g_{k+1} + \beta_k d_k^TAd_k,
\end{align}
а $\beta_k$ находится из требования
$$d_{k+1}^TAd_k = -g_{k+1}^TAd_k + d_k^TAd_k \Rightarrow \beta_k = \frac{g_{k+1}^TAd_k}{d_k^TAd_k},$$
и впрямь является набором сопряженных векторов относительно матрицы $A$. Следующая теорема это доказывает.

\begin{Theorem}
Если $d_k$ построены по правилу \eqref{cgrad}, то для любого $0\leq k\leq n-1$ верно:
\begin{enumerate}
    \item $d_0, d_1, \dots, d_k \in \langle g_0, Ag_0, \dots, A^k g_0 \rangle$ \\ $g_0, g_1, \dots, g_k \in \langle g_0, Ag_0, \dots, A^k g_0 \rangle$;
    \item $g_k^Tg_j = 0$ для любых $j \neq k$;
    \item $d_k^TAd_j = 0$ для любых $j \neq k$.
\end{enumerate}
\end{Theorem}
\begin{proof}
Заметим, что нам достаточно доказать последние два свойства немного в другой форме, а именно: $g_k^Tg_j = 0$ для любых $j < k$ и $d_k^TAd_j = 0$ для любых $j < k$. Доказательство будем вести по индукции.

При $k = 0$ первое свойство очевидно: $g_0 \in \langle g_0 \rangle$, $d_0 = -g_0 \in \langle g_0 \rangle$, а оставшиеся два выполнены бессодержательно. Пусть теперь утверждение теоремы верно при $k$. Рассмотрим случай для $k+1$.
\begin{enumerate}
    \item $g_{k+1} = Ax_{k+1} - b = A(x_k + \alpha_kd_k) - b = (Ax_k - b) + \alpha_kAd_k = g_k + \alpha_k Ad_k$. По предположению, $g_k \in \langle g_0, Ag_0, \dots, A^k g_0 \rangle$, $d_k \in \langle g_0, Ag_0, \dots, A^k g_0 \rangle$. Это означает, что $Ad_k \in \langle Ag_0, A^2g_0, \dots, A^{k+1} g_0 \rangle$, и $g_{k+1} \in \langle g_0, Ag_0, \dots, A^{k+1} g_0 \rangle$.
    
    Теперь заметим, что $d_{k+1} = -g_{k+1} + \beta_kd_k$. Из уже полученного результата заключаем, что $d_{k+1} \in \langle g_0, Ag_0, \dots, A^{k+1} g_0 \rangle$. Подпространства вида $\langle g_0, Ag_0, \dots, A^{k} g_0 \rangle$ называются \textit{подпространствами Крылова}.
    \item Заметим, что $d_0, \dots, d_k$ --- базис в подпространстве $\langle g_0, Ag_0, \dots, A^{k} g_0 \rangle$. Действительно, по предположению индукции, $d_i^TAd_j = 0$ при $i \neq j$, $i, j \leq k$, а значит эти векторы линейно независимы в силу утверждения, которое мы уже доказывали. Это означает, что $g_j = \sum\limits_{i=0}^{j-1}\gamma_id_i$. Тогда при $j < k+1$:
    $$g_{k+1}^Tg_j = g_{k+1}^T\left(\sum_{i=0}^{j-1}\gamma_id_i\right) = \sum_{i=0}^{j-1} \gamma_i \underbrace{g_{k+1}^Td_i}_{=0} = 0$$
    Мы воспользовались тем, что градиент ортогонален всем предыдущим направлениям (доказательство этого факта мы привели в начале лекции).
    \item Заметим, что мы имеем $d_{k+1}^TAd_k = 0$ по построению (мы соответствующим образом выбирали $\beta_k$). Если $j < k$, то
    $$d_{k+1}^TAd_j = (g_{k+1} + \beta_kd_k)^TAd_j = g_{k+1}^TAd_j + \beta_k \underbrace{d_k^TAd_j}_{=0} = g_{k+1}^TAd_j = (*)$$
    Заметим теперь, что $g_{j+1} = g_j + \alpha_jAd_j \Rightarrow Ad_j = \frac{g_{j+1} - g_j}{\alpha_j}$ и 
    $$(*) = g^T_{k+1}\frac{g_{j+1} - g_j}{\alpha_j} = 0$$
    по предыдущему пункту, так как мы рассматриваем случай $j < k$. Итак, при $j < k+1$ верно $d_{k+1}^TAd_j = 0$.
\end{enumerate}
\end{proof}

Для метода сопряженных градиентов можно немного модифицировать формулы для $\alpha_k$ и $\beta_k$:
$$\alpha_k = -\frac{g_k^Td_k}{d_k^TAd_k} = -\frac{g_k^T(-g_k + \beta_kd_{k-1})}{d_k^TAd_k} = \frac{g_k^Tg_k}{d_k^TAd_k}$$
$$\beta_k = \frac{g_{k+1}^TAd_k}{d_k^TAd_k} = \frac{g_{k+1}^T(g_{k+1} - g_k)}{d_k^TAd_k \cdot alpha_k} = \frac{g_{k+1}^Tg_{k+1}}{g_k^Tg_k}$$

\begin{algorithm}[t]
\caption{Метод сопряженных градиентов}\label{alg:CG}
\begin{algorithmic}[1]
\State $g_0 \gets Ax_0 - b$
\State $d_0 \gets -g_0$
\For{$k = 0, \dots , 2n-1$}
    \State $\alpha_k = \frac{g_k^Tg_k}{d_k^TAd_k}$
    \State $x_{k+1} = x_k + \alpha_kd_k$
    \State $g_{k+1} = Ax_{k+1} - b$
    \If {$\|g_{k+1}\|^2 \leq \varepsilon$} \Then
        \State\textbf{break}
    \EndIf
    \State $\beta_k = \frac{g_{k+1}^Tg_{k+1}}{g_k^Tg_k}$
    \State $d_{k+1} = -g_{k+1} + \beta_kd_k$
\EndFor
\end{algorithmic}
\end{algorithm}

Если внимательно присмотреться к алгоритму \ref{alg:CG}, можно заметить, что тело цикло выполняется не $n$ раз, а $2n$ раз. Это связано с тем, что существуют вычислительные неточности, и мы, вообще говоря, не гарантируем, что численно алгоритм сойдётся за $n$ итераций. Однако мы надеемся, что ему хватит $2n$ итераций.

Посмотрим, сколько и каких операций выполняется в алгоритме \ref{alg:CG}. Первое, что мы замечаем, так это то, что нам нам на каждой итерации будет достаточно выполнить одну операцию умножения матрицы на вектор, стоимость которой $O(n^2)$. Действительно, до выполнения цикла мы уже посчитали $Ax_0$. Теперь давайте на каждой итерации вычислять лишь $Ad_k$ и хранить в памяти $Ax_k$ с предыдущей итерации. Тогда мы сможем вычислить $Ax_{k+1}$ без перемножения матриц: $Ax_{k+1} = Ax_k + \alpha_kAd_k$, и мы знаем каждое слагаемое.

Нам будет достаточно два раза за итерацию вычислить произведение векторов $x^Ty$, которая выполняется за $O(n)$. Опять же, мы можем хранить $g_k^Tg_k$ с предыдущей итерации. Нам также потребуется три раза за итерацию сложить векторы за $O(n)$.

Итак, за итерацию мы совершим примерно $n^2 + 5n$ действий. А это означает, что общее количество действий составит примерно $2n^3 + 10n^2$. Тут пора остановиться и задуматься, ведь разложение Холецкого работает примерно за $n^3/3$ операций. Оказывается, наши старания были не напрасны.

Мы будем применять метод сопряженных градиентов внутри метода Ньютона, чтобы отыскать направление спуска $-[\nabla^2f(x_k)]^{-1}\nabla f(x_k)$. В следующей лекции мы узнаем, что нам не обязательно вычислять гессиан и хранить его в памяти (что очень дорого: $O(n^2)$), и мы можем находить произведение гессиана на вектор, не зная сам гессиан.

\end{document}
