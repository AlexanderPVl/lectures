\documentclass[a4paper, 12pt]{article}
%\input{header.sty}
%\usepackage{tikz}
%\usepackage{algorithm}
%\usepackage[noend]{algpseudocode}


\begin{document}
%\maketitle

\section{Лекция 4}

Вспомним конец предыдущей лекции, когда мы пытались оценить скорость сходимости метода градиентного спуска при минимизации функции $f \in C^{1, 1}_{L}$. Промежуточным результатом наших рассуждений было неравенство
$$f(x_{k+1}) \leq f(x_k) - \frac{1}{2L}\|\nabla f(x_k)\|^2,$$
из которого впоследствии мы получили, что для $\mu$-сильно выпуклых функций справедливо:
$$r_{k+1} \leq \left(1 - \frac{\mu}{L}\right)r_k,$$
где $r_k = f(x_k) - f(x_{\text{opt}})$ --- невязка по значению функции. После этого мы сказали, что метод градиентного спуска для $\mu$-сильно выпкулых функции имеет линейную скорость сходимости. 

Представим теперь, что для какого-то метода оптимизации справедливо неравенство, аналогичное неравенству для метода градиентного спуска, а именно
$$f(x_{k+1}) \leq f(x_k) - \mathrm{const} \cdot \|\nabla f(x_k)\|^2.$$
Тогда, повторяя аналогичные рассуждения, получим, что для $\mu$-сильно выпукулых функций верно
$$r_{k + 1} \leq (1 - 2\mu \cdot  \mathrm{const})r_k.$$
Этот результат нам еще понадобится.

Вспомним, что в методе градиентного спуска в качестве величины шага мы использовали $\alpha_k = 1/L$, но встаёт вопрос: как нам оценить константу Липшица $L$? Первое, что стоит заметить --- это то, что нам, вообще говоря, не интересна глобальная константа Липшица, а только лишь её локальная оценка. Действительно, рассмотрим функцию $f$, график которой показан на рисунке. Пусть мы ищем длину шага в окрестности точки $B$.
 
\begin{center}
\begin{tikzpicture}
\draw[thick,->] (0,0) -- (5,0) node[anchor=north] {$x$};
\draw[thick,->] (0,0) -- (0,4.5) node[anchor=east] {$f(x)$};
\draw[domain=0.25:1.5,smooth,variable=\x,thick] plot ({\x},{1/\x});
\draw[domain=3.5:4.75,smooth,variable=\x,thick] plot ({\x},{1/(5-\x)});
\draw[domain=1.5:3.5,smooth,variable=\x,thick] plot ({\x},{0.2222 * \x^2 - 1.1111 * \x + 1.8333});
\draw[domain=0.25:0.49,smooth,variable=\x,color=red,thick] plot ({\x},{80 * \x^2 - 59.1 * \x + 13.86});
\draw[domain=0.95:2.4,smooth,variable=\x,color=red,thick] plot ({\x},{1 * \x^2 - 3.44 * \x + 3.58});
\node[circle,fill=black,inner sep=0pt,minimum size=4pt,label=right:{$A$}] (a) at (0.3,3.333) {};
\node[circle,fill=black,inner sep=0pt,minimum size=4pt,label=above:{$B$}] (b) at (1.5,0.6666) {};
\end{tikzpicture}
\end{center}

На интервале, на котором задан график функции, градиент $\nabla f(x)$ липшицев: в любой точке можно построить такие параболы с некоторыми кривизнами $-L$ и $L$, что график будет лежать строго между этими параболами. Это означает, что в том числе и параболы, построенные в точке $A$, должны ограничивать график. Из рисунка видно, что кривизны парабол должны быть очень велики по модулю, т.е. глобальная константа Липшица $L \gg 1$, и шаг градиентного спуска $\alpha_k = 1 / L$ близок к нулю.

\begin{algorithm}
\caption{Подбор $L$}\label{alg:L}
\begin{algorithmic}[1]
\State $L \gets L_0$
\Loop
    \State $x_{k+1} \gets x_k - \frac{1}{L} \nabla f(x_k)$
    \If {$f(x_{k+1}) \leq f(x_k) + \nabla f(x)^T (x_{k+1} - x_k) + \frac{L}{2}\|x_{k+1} - x_k\|^2$} \Then
    \State\textbf{break}
    \EndIf
    \State $L \gets \gamma L$, $\gamma > 1$
\EndLoop
\State $L \gets \rho L$, $\rho < 1$
\end{algorithmic}
\end{algorithm}

С другой стороны, мы видим, что в окрестности точки $B$ график функции пологий, а значит локальная константа Липшица для этой окрестности невелика, и $L_{\text{лок}} \ll L_{\text{глоб}}$, и шаг величины $1/L_{\text{лок}}$ уже не так близок к нулю. В приведенном алгоритме \ref{alg:L} поиска константы Липшица используется эта идея: вместо нахождения точной константы Липшица можно ограничиться её некоторой локальной оценкой.

Попытаемся разобраться в алгоритме. Мы начинаем с приближения $L_0$, а затем на каждой итерации цикла совершаем один градиентный шаг. Если для пары точек $x_{k}$ и $x_{k+1}$ выполнено условие липшицевости для градиента, выходим из цикла, так как мы нашли некоторое локальное приближение константы Липшица. Иначе увеличим текущее приближение в $\gamma > 1$ раз и попробуем снова. После выхода из цикла уменьшим оценку в $\rho < 1$ раз.

\subsection{Глобальная сходимость методов спуска}
Напомним, что метод спуска задается соотношением:
$$x_{k + 1} = x_k + \alpha_k d_k, \; \nabla f(x_k)^T d_k < 0,$$
а $\alpha_k$ удовлетворяет условиям Армихо--Вульфа:
\begin{equation}
    f(x_{k+1}) \leq f(x_k) + c_1 \alpha_k \nabla f(x_k)^T d_k \label{armijo}
\end{equation}
\begin{equation}
    \nabla f(x_{k+1})^T d_k \geq c_2 \nabla f(x_k)^T d_k \label{wolfe}
\end{equation}

Теперь давайте попытаемся оценить значение функции $f(x_{k+1})$ сверху через $f(x_k)$ и $\nabla f(x_k)$. Мы хотим получить неравенство, которое обсуждалось в начале лекции, а именно:
$$f(x_{k+1}) \leq f(x_k) - \mathrm{const} \cdot \|\nabla f(x_k)\|^2.$$

Сперва рассмотрим следующую цепочку неравенств:
\begin{align*}
    \left(\nabla f(x_{k+1}) - \nabla f(x_k)\right)^T(x_{k+1} - x_k) &\leq \|\nabla f(x_{k+1}) - \nabla f(x_k)\| \cdot \|x_{k+1} - x_k\| \\ &\leq L \|x_{k+1} - x_k\|^2 = L \alpha_k^2 \cdot \|d_k\|^2. 
\end{align*}
Здесь мы воспользовались неравенством Коши-Буняковского и тем, что $f \in C^{1, 1}_{L}$. С другой стороны, то же самое выражение можно оценить снизу:
\begin{align*}
    \left(\nabla f(x_{k+1}) - \nabla f(x_k)\right)^T(x_{k+1} - x_k) &\geq \text{\{неравенство \eqref{wolfe}\}} \geq (c_2 - 1) \nabla f(x_k)^T(x_{k+1} - x_k) \\ &= (c_2 - 1) \alpha_k \nabla f(x_k)^Td_k.
\end{align*}

Отсюда мы заключаем, что 
$$L \alpha_k^2 \cdot \|d_k\|^2 \geq (c_2 - 1) \alpha_k \nabla f(x_k)^Td_k \Rightarrow \alpha_k \geq \frac{(c_2 - 1) \nabla f(x_k)^T d_k}{L \cdot \|d_k\|^2}.$$

Обозначим за $\theta_k$ угол между $d_k$ и $\nabla f(x_k)$, т.е.
$$\cos \theta_k = \frac{\nabla f(x_k)^T d_k}{\|\nabla f(x_k)\| \cdot \|d_k\|}.$$

Теперь из неравенства \eqref{armijo} и выведенной нами оценки $\alpha_k$ получаем:
$$f(x_{k+1}) \leq f(x_k) + \frac{c_1(c_2 - 1) \left(\nabla f(x_k)^T d_k \right)^2}{L \cdot \|d_k\|^2} = f(x_k) - \frac{c_1(1 - c_2) \cos^2 \theta_k}{L} \cdot \|\nabla f(x_k)\|^2.$$

Если мы сможем требовать, чтобы $\cos \theta_k$ был отделен от нуля некоторой константой $\delta$, получим, что 
$$f(x_{k+1}) \leq f(x_k) - \underbrace{\frac{c_1(1 - c_2) \delta^2}{L}}_{\mathrm{const}} \cdot \|\nabla f(x_k)\|^2.$$

Итак, при использовании методов линейного спуска с условиями Армихо-Вульфа на шаг $\alpha_k$ мы можем гарантировать линейную скорость сходимости в том случае, когда $\cos \theta_k$ отделён от нуля.

\subsection{Основные матричные разложения}
В курсе часто будут возникать задачи решения СЛАУ $Ax = b$, которые численно нельзя решать как $x = A^{-1}b$, поскольку это решение будет численно неустойчивым. При решении СЛАУ обычно пользуются основными матричными разложениями, с которыми мы сейчас познакомимся.

Напомним, что существуют матрицы элементарных преобразований $U$: это единичные матрицы, над которыми были произведены эти элементарные преобразования. При умножении на матрицу слева элементарные преобразования производятся над строками матрицы $A$, при умножении справа --- над столбцами. Нас будут инетерсовать лишь перестановочные матрицы, которые мы будем обозначать $P$.

\begin{enumerate}
    \item \textbf{Разложение Холецкого}. Пусть $A = A^T > 0$ --- симметричная положительно определенная матрица. Тогда найдется такая нижнетреугольная матрица $L$ со строго положительными элементами на диагонали, что $A = LL^T$. Пусть теперь мы хотим решить систему линейных уравнений $Ax = b$; после разложения Холецкого получим $LL^T x = b$.
    
    Пусть теперь $y = L^Tx$. Заметим, что система $Ly = b$ решается очень легко, ведь матрица $L$ нижнетреугольная, и нам не придется явно обращать матрицу (вспомним, что в методе Гаусса мы пытаемся приводить матрицу к верхнетреугольному виду, после чего решение ищется легко; то же справедливо и в случае нижнетреугольной матрицы). После нахождения $y$ найдем $x$, решив СЛУ $L^Tx = y$ с верхнетреугольной матрицей $L^T$.
    
    Разложение Холецкого имеет сложность $O\left(\frac{n^3}{3}\right)$.
    
    \item \textbf{Модифицированное разложение Холецкого} ($LDL$-разложение). Пусть матрица $A = A^T$ симметричная, но уже не обязательно положительно определенная. Тогда найдутся такие перестановочная матрица $P$, нижнетреугольная матрица $L$ с единицами на диагонали и блочно-диагональная матрица $D$ с блоками размера $1\times 1$ и $2 \times 2$, что $P^TAP = LDL^T$ (говорят, что разложение справедливо с точностью до перестановочной матрица). Модифицированное разложение Холецкого также позволяет численно устойчиво решать системы линейных уравнений, рассуждения ведутся так же, как и в предыдущем случае.
    
    Стоимость разложения: $O\left(\frac{n^3}{3}\right)$ без вычисления перестановки.
    
    \item \textbf{$LU$-разложение}. Пусть матрица $A$ квадратная, но уже не обязательно симметричная. Тогда найдутся такие перестановочная матрица $P$, нижнетреугольная матрица $L$ и верхнетреугольная $U$, что $PA = LU^T$.
    
    Стоимость разложения: $O\left(\frac{2n^3}{3}\right)$.
    
    \item \textbf{$QR$-разложение}. Пусть $A \in \R^{n \times m}$. Тогда найдется перестановочная матрица $P$, ортогональная матрица $Q$ (т.е. $Q^T = Q^{-1}$) и верхнетреугольная прямоугольная матрица $R$, что $AP = QR$.
    
    Пусть имеется несовместная система $Ax = b$, $n > m$. Так как найти точное решение не представляется возможным, ищут псевдорешение, т.е. $$x = \arg\min_{x} \|Ax - b\|$$ Эта задача нам очень хорошо знакома, и мы даже знаем решение $x = (A^TA)^{-1}A^Tb$. Разложим матрицу $A$ в $QR$-разложение, ради простоты предполагая, что $P = I_n$. Тогда $A = QR$, а матрицы $Q$ и $R$ имеют блочно-матричный вид
    $$R = \left( \begin{matrix}
        R_1 \\
        0
    \end{matrix}\right), \; Q = \left(\begin{matrix}
    Q_1 & Q_2 
    \end{matrix} \right), \; QR = Q_1R_1 + Q_2\cdot 0 = Q_1R_1$$
    где $R_1$ --- квадратная верхнетреугольная матрица (невырожденная, что нам дальше понадобится).
    
    Тогда
    \begin{align}
        x &= (A^TA)^{-1}A^Tb = (R^TQ^TQR)^{-1}R^TQ^Tb \\
          &= (R^TR)^{-1}R^TQ^Tb = (R_1^TR_1)^{-1} R_1^TQ_1^Tb \\
          &= R_1^{-1}Q_1^{T}b
    \end{align}
    и хоть мы и пишем $R_1^{-1}$, на самом деле это означает, что численно мы решаем систему уравнений $R_1x = Q_1^Tb$ с верхнетреугольной матрицей $R_1$.
    
    Стоимость $QR$-разложения: $O\left(\frac{4n^3}{3}\right)$.
    
\end{enumerate}

\subsection{Метод Ньютона}
Рассмотрим класс методов оптимизации, которые приближают функцию $f$ следующим образом:
$$f(x_k + d) \approx m_k(d) = f(x_k) + \nabla f(x_k)^Td + \frac{1}{2} d^T B_k d$$
где $B_k = B_k^T > 0$ --- симметричная положительно определенная матрица.

Будем искать $d$, минимизирующее нашу модель $m_k(d)$ (и косвенно минимизирующее $f(x_k + d)$). Для этого приравняем градиент к нулю:
$$\nabla_d m_k(d) = 0 \Leftrightarrow \nabla f(x_k) + B_k d = 0 \Leftrightarrow d = -B_k^{-1}\nabla f(x_k)$$

Заметим, что $d$ в таком случае будет направлением спуска, т.к. $$\nabla f(x_k)^Td = -\nabla f(x_k) B_k^{-1}\nabla f(x_k) < 0$$ в силу положительной определенности матрицы $B_k$.

Видим, что в случае $B_k = I_n$ мы получим метод градиентного спуска. В случае же $B_k = \nabla^2 f(x_k)$ метод носит названия \textit{метода Ньютона}. Заметим, что гессиан, вообще говоря, не обязан быть положительно определенным, поэтому в алгоритме производится коррекция гессиана до положительно определенной матрицы.

Итак, один шаг метода Ньютона имеет вид
$$x_{k+1} = x_k - \alpha_k \left[\nabla^2 f(x_k)\right]^{-1} \nabla f(x_k).$$

Рассмотрим матрицу перехода $A$ к новому базису. Если за $\tilde{x}$ обозначить координаты вектора в новом базисе, $x = A\tilde{x}$. Пусть теперь $\tilde{f}(\tilde{x}) = f(x) = f(A\tilde{x})$. Тогда
\begin{align}
    \nabla \tilde{f}(\tilde{x}) &= A^T \nabla f(x) \\
    \nabla^2 \tilde{f}(\tilde{x}) &= A^T \nabla^2 f(x) A.
\end{align}

Сделаем один шаг метода Ньютона в новых координатах:
\begin{align}
    \tilde{x}_{k+1} &= \tilde{x}_k - \alpha_k \left[\nabla^2 \tilde{f}(\tilde{x}_k)\right]^{-1} \nabla \tilde{f}(\tilde{x}_k) \\  &= \tilde{x}_k - \alpha_k A^{-1}\left[\nabla^2 f(x_k)\right]^{-1} A^{-T}A^T \nabla f(x_k) \\ &= \tilde{x}_k - \alpha_k A^{-1}\left[\nabla^2 f(x_k)\right]^{-1}\nabla f(x_k).
\end{align}
Умножив обе части уравнения на $A$, получим
$$x_{k+1} = x_k - \alpha_k \left[\nabla^2 f(x_k)\right]^{-1} \nabla f(x_k),$$
т.е. один шаг метода Ньтона в новых координатах соответствует шагу метода Ньютона в старых координатах, или траектория $x$ равна траектории $\tilde{x}$. Заключаем, что \textit{метод Ньютона инвариантен к шкалированию координат}. Отсюда видно, что метод Ньютона, в отличие от метода градиентного спуска, не имеет проблем с функциями с вытянутыми линиями уровня. % TODO: последнее предложение откровенно плохо

\subsection{Скорость сходимости метода Ньютона}
Метод Ньютона, как и метод парабол, имеет квадратичную локальную скорость сходимости и линейную глобальную скорость сходимости. Докажем это.

Заметим, что метод Ньютона является методом спуска. Вспомним, что мы показали, что если $\alpha_k$ удовлетворяет условиям Армихо--Вульфа, а косинус угла между $d_k$ и $\nabla f(x_k)$, который мы обозначали $\cos \theta_k$, отделен от нуля константой, мы можем гарантировать линейную сходимость метода оптимизации. Покажем, что в случае метода Ньютона $\cos \theta_k$ действительно отделен от нуля.

Но для начала вспомним несколько фактов из линейной алгебры. Во-первых, всякую положительно определенную квадратичную форму $Q$ можно привести к диагональному виду с помощью ортогональной матрицы $C$. Вспомним также, что все собственные значения положительно определенной матрицы положительны. Пользуясь этим, запишем 
\begin{align}
    x^TQx &= (C\tilde{x})^T Q (C\tilde{x}) = \tilde{x}^T (C^T Q C) \tilde{x} \\ &= \lambda_1 \tilde{x}_1^2 + \dots + \lambda_n \tilde{x}_n^2 \geq \lambda_{\mathrm{min}} (\tilde{x}_1^2 + \dots + \tilde{x}_n^2) \\ &= \lambda_{\mathrm{min}} \|\tilde{x}\|^2 = \lambda_{\mathrm{min}} \|x\|^2
\end{align}
так как ортогональное преобразование сохраняет норму. Очевидно, равенство достигается. Аналогично можно показать, что $x^T Q x \leq \lambda_{\mathrm{max}} \|x\|^2$ (опять же, равенство достигается).

Вспомним, что для положительно определенной квадратичной формы $Q$ её операторная норма $\|Q\|_{\mathrm{op}} = \lambda_{\mathrm{max}}$ и из согласованности матричной и векторной норм следует, что $\|Qx\| \leq \|Q\|_{\mathrm{op}} \|x\| = \lambda_{\mathrm{max}}\|x\| $.

Так как $B_k$ --- положительно определенная симметричная матрица, таковой является и $B_k^{-1}$. Тогда в силу вышеизложенного
$$
    \cos \theta_k = \frac{\nabla f(x_k)^T d_k}{\|\nabla f(x_k)\| \cdot \|d_k\|} = - \frac{\nabla f(x_k)^T B^{-1}_k \nabla f(x_k)}{\|\nabla f(x_k)\| \cdot \| B^{-1}_k \nabla f(x_k)\|} \leq -\frac{\lambda_{\mathrm{min}}(B_k^{-1})}{\lambda_{\mathrm{max}}(B_k^{-1})} < 0
$$

Итак, мы показали, что метод Ньютона имеет глобальную линейную скорость сходимости. Докажем, что локально он имеет квадратичную скорость сходимости.


Пишут, что $A \geq B$, если $A - B \geq 0$ в смысле неотрицательной определенности. При этом оказывается, что $A^{-1} \leq B^{-1}$, а $\|A\| \geq \|B\|$.


\begin{Statement}
Пусть $f \in C^{2,2}_M$, $\nabla^2f(x_{\mathrm{opt}}) \geq \mu I$, $\alpha_k = 1$, а $x_0$ такой, что $\|x_0 - x_{\mathrm{opt}}\| \leq r$. Тогда для метода Ньютона верно, что $\|x_{\mathrm{opt}} - x_{k+1}\| \leq \mathrm{const} \cdot \|x_{\mathrm{opt}} - x_k\|^2$.
\end{Statement}
\begin{proof}
Заметим, что в случае $\alpha_k = 1$, $x_{k+1} = x_k - \left[\nabla^2 f(x_k)\right]^{-1} \nabla f(x_k)$, а $\nabla f(x_{\mathrm{opt}}) = 0$ в силу необходимого условия для оптимальности. Тогда справедливы равенства
\begin{align}\|x_{k+1} - x_{\mathrm{opt}}\| &= \left\|x_k - x_{\mathrm{opt}} - \left[\nabla^2 f(x_k)\right]^{-1} \nabla f(x_k)\right\| \\
&= \left\|\left[\nabla^2 f(x_k)\right]^{-1} \left(\nabla^2 f(x_k)(x_k - x_{\mathrm{opt}}) -  \nabla f(x_k)\right) + 0\right\| \\
&= \left\|\left[\nabla^2 f(x_k)\right]^{-1} \left(\nabla^2 f(x_k)(x_k - x_{\mathrm{opt}}) -  (\nabla f(x_k) - \nabla f(x_{\mathrm{opt}}))\right) \right\| = (*)
\end{align}
Из теоремы Ньютона--Лейбница следует, что 
$$\nabla f(x_k) - \nabla f(x_{\mathrm{opt}}) = \int_0^1 \nabla^2 f(x_{\mathrm{opt}} + \tau (x_k - x_{\mathrm{opt}})) (x_k - x_{\mathrm{opt}}) d\tau$$
а значит 
\begin{align} (*)
&= \left\|\left[\nabla^2 f(x_k)\right]^{-1} \int_0^1 \left(\nabla^2 f(x_k) -  \nabla^2 f(x_{\mathrm{opt}} + \tau (x_k - x_{\mathrm{opt}}))\right) (x_k - x_{\mathrm{opt}}) d\tau \right\| \\
& \leq \left\|\left[\nabla^2 f(x_k)\right]^{-1} \right\| \cdot \left\|\int_0^1 \left(\nabla^2 f(x_k) -  \nabla^2 f(x_{\mathrm{opt}} + \tau (x_k - x_{\mathrm{opt}}))\right) (x_k - x_{\mathrm{opt}}) d\tau \right\| \\
& \leq \left\|\left[\nabla^2 f(x_k)\right]^{-1} \right\| \cdot \int_0^1 \left\|\left(\nabla^2 f(x_k) -  \nabla^2 f(x_{\mathrm{opt}} + \tau (x_k - x_{\mathrm{opt}}))\right) \right\| \cdot \left\|x_k - x_{\mathrm{opt}}\right\| d\tau \\
& \leq \text{\{в силу липшицевости гессиана\}} \\
& \leq \left\|\left[\nabla^2 f(x_k)\right]^{-1} \right\| \cdot M\|x_k - x_{\mathrm{opt}}\|^2 \int_0^1(1-\tau)d\tau \\ &= \frac{M}{2} \left\|\left[\nabla^2 f(x_k)\right]^{-1} \right\| \cdot \|x_k - x_{\mathrm{opt}}\|^2
\end{align}
Так как $\nabla^2f(x_{\mathrm{opt}}) \geq \mu I$, получаем, что $\left\|\left[\nabla^2 f(x_{\mathrm{opt}})\right]^{-1} \right\| \leq \left\|(1/\mu) I \right\|$. В силу непрерывности нормы $\|\cdot\|$ и гессиана, получим, что коль скоро $\|x_0 - x_{\mathrm{opt}}\| \leq r$, $\left\|\left[\nabla^2 f(x_k)\right]^{-1} \right\| \leq \mathrm{const}$. В итоге, заключаем
$$\|x_{\mathrm{opt}} - x_{k+1}\| \leq C \cdot \|x_{\mathrm{opt}} - x_k\|^2$$
В домашней работе мы доказывали, что сходимость будет наблюдать в том и только том случае, когда $C \cdot \|x_0 - x_\mathrm{opt}\| < 1$, т.е. когда $r < 1/C$.
\end{proof}
\end{document}