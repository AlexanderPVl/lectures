\documentclass[a4paper, 12pt]{article}
%\input{header.sty}
%\usepackage{algorithm}
%\usepackage[noend]{algpseudocode}

\begin{document}
%\maketitle
\section{Лекция 6}

\subsection{Метод Ньютона для невыпуклых функций}
В лекции 4, ведя разговор о методе Ньютона, мы сначала рассмотрели некоторый класс методов, в которых направление спуска задаётся как $B_k^{-1}\nabla f(x)k$, где $B_k = B_k^T > 0$ --- симметричная положительно определенная матрица. Мы показали, что независимо от того, какой будет матрица $B_k$, пока она остается симметричной и положительно определенной, метод будет иметь глобальную линейную скорость сходимости. Затем мы рассмотрели случай $B_k = \nabla^2 f(x_k)$ (метод Ньютона) и доказали его локальную квадратичную сходимость.

Всё бы хорошо, но гессиан функции не обязан быть положительно определённым. Ниже мы рассмотрим подходы, которые позволят нам корректировать гессиан невыпуклых функций до положительно определённой матрицы.

\textbf{Первый способ} --- способ грубой силы. Гессиан --- симметричная матрица, а значит её можно разложить как $\nabla^2 f(x_k) = Q^T\Lambda Q$, где $Q$ --- ортогональная матрица, а $\Lambda = \diag(\lambda_1, \dots, \lambda_n)$ --- матрица собственных значений. Мы хотим сделать все собственные значения матрицы больше нуля, ведь матрица положительно определена тогда и только тогда, когда все её собственные значения отделены от нуля. Давайте преобразуем спектр матрицы следующим образом:
$$
\tilde{\lambda}_i = \begin{cases}
    \lambda_i,&\text{если } \lambda_i \geq \delta \\
    \delta,&\text{иначе}
\end{cases}
$$
где $\delta > 0$. Откорректированный гессиан тогда запишется как $B_k = Q^T\tilde{\Lambda}Q$, где $\tilde{\Lambda} = \diag({\tilde{\lambda}_1, \dots, \tilde{\lambda}_n})$. Отметим, что имея собственное разложение гессиана, нам не придётся вычислять, а затем его обращать. Действительно, мы решаем систему $$B_kd_k = -\nabla f(x_k) \Leftrightarrow  Q^T\underbrace{\tilde{\Lambda}\overbrace{Qd_k}^{z}}_{y} = -\nabla f(x_k)$$
откуда $y = -Q^{-T}\nabla f(x_k) = -Q\nabla f(x_k)$ в силу ортогональности матрицы $Q$, $z = -\Lambda^{-1}Q\nabla f(x_k)$, а обращать диагональную матрицу очень легко: достаточно обратить каждый её диагональный элемент. И, опять же, в силу ортогональности $Q$ получим $d_k = -Q^T\Lambda^{-1}Q\nabla f(x_k)$.

\textbf{Второй подход} заключается в модификации диагонали гессиана. Для начала рассмотрим произвольную матрицу $A$ и число $\tau$. Пусть спектр матрицы $A$ равен $\{\lambda_1, \dots, \lambda_m\}$. Тогда спектр матрицы $A + \tau I$ равен $\{\lambda_1 + \tau, \dots, \lambda_m + \tau\}$. Действительно, пусть $v_i$ --- собственный вектор $A$, соответствующий собственному значению $\lambda_i$, тогда:
$$(A + \tau I)v_i = Av_i + \tau v_i = \lambda_i v_i + \tau v_i = (\lambda_i + \tau) v_i$$
т.е. $v_i$ --- это собственный вектор матрицы $A + \tau I$ с собственным значением $\lambda_i + \tau$. В обратную сторону доказательства аналогичны. 

Теперь пусть $\tau_k > \max\left\{0, -\lambda_\mathrm{min}(\nabla^2 f(x_k))\right\}$. Тогда из показанного выше следует, что матрица $\nabla^2 f(x_k) + \tau_kI > 0$. Итак, направление спуска
$$d_k = -\left(\nabla^2 f(x_k) + \tau_k I\right)^{-1} \nabla f(x_k)$$
Если $\tau_k \gg 1$, в сумме бОльшую роль играет второе слагаемое, а значит $d_k \approx -(1/\tau_k) \nabla f(x_k)$ по направлению практически совпадает с направлением градиентного спуска. Если же $\tau_k \approx 0$, второе слагаемое практически не вносит вклада, и $d_k \approx -[\nabla^2 f(x_k)]^{-1}\nabla f(x_k)$. Итак, чем меньше $\tau_k$, тем лучше, ведь мы хотим сохранить локальную квадратичную скорость сходимости метода Ньютона и не превращать его в градиентный спуск. Алгоритм \ref{alg:tau} производит подбор $\tau_k$ пользуясь тем, что разложение Холецкого существует тогда и только тогда, когда матрица симметрична и положительна определена (функция разложения вернет ошибку, если не сможет найти разложение).

\begin{algorithm}[t]
\caption{Подбор $\tau_k$}\label{alg:tau}
\begin{algorithmic}[1]
\Loop
    \State $B_k \gets \nabla^2f(x_k) + \tau_k I$
    \State $L \gets \mathrm{cholesky}(B_k)$ \Comment{пытаемся найти разложение Холецкого}
    \If{$1$} \Then
        \State\textbf{break}
    \EndIf
    \State $\tau_k = \gamma \tau_k$, $\gamma > 1$
\EndLoop
\State $\tau_{k+1} \gets \rho \tau_k$, $\rho < 1$
\end{algorithmic}
\end{algorithm}

\textbf{Третий подход} использует модифицированное разложение Холецкого. Найдутся такие перестановочные матрица $P$, нижнетреугольная матрица $L$ с единицами диагонали и блочно-диагональная симметричная матрица $D$ с блоками размером $1\times1$ и $2\times2$, что $P^T \nabla^2 f(x_k) P = LDL^T$. В силу того, что матрица $D$ симметрична, мы можем найти её собственное разложение $Q\Lambda Q^T$. Теперь, как в первом способе, преобразуем матрицу $\Lambda$ к $\tilde{\Lambda}$. Обозначим $\tilde{D} = Q^T\tilde{\Lambda}Q$. Так как перестановочные матрица ортогональны, откорректированный гессиан запишется как $B_k = PL\tilde{D}L^TP^T$.

\subsection{Hessian-Free Newton}
\textit{Альтернативные названия}: Newton-CG, Inexact Newton, Truncated Newton.

Запишем, как мы минимизируем функцию методом Ньютона:
\begin{gather*}
    f(x) \to \min_x \\
    x_{k+1} = x_k + \alpha_k d_k \\
    d_k = -B_k\nabla f(x_k)
\end{gather*}
Мы уже привыкли к тому, что нам необходимо не обращать матрицу $B_k$, а решать СЛАУ $B_kd_k = -\nabla f(x_k)$. Горький опыт учит нас, что мы не хотим вычислять никаких матриц-гессиан, но хотим уметь решать такую систему уравнений. К счастью, у нас есть метод сопряжённых градиентов, который требует от нас лишь умения умножать матрицу $B_k$ на произвольный вектор, и не требует её вычисления. Умножать гессиан на вектор можно, не вычисляя гессиан напрямую (о том, как это сделать, см. ниже). Применение метода сопряженных градиентов --- первая хорошая идея.

Вторая идея заключается в том, что мы поначалу можем решать СЛАУ неточно: если мы нашли близкое к оптимальному $d_k$, то шагнём примерно в правильном направлении. Однако надо подчеркнуть, что делаем так мы только когда находимся далеко от точки оптимума: когда мы отстоим от решения недалеко, нам всё же придётся искать точное направление спуска $d_k$, иначе мы можем блуждать около точки минимума сколь угодно долго. Сформулированная эвристика поможет нам сократить время работы алгоритма. Осталось лишь формализовать, что мы будем считать неточным решением СЛАУ.

За невязку решения СЛАУ $B_k d_k = -\nabla f(x_k)$ обозначим $r_k = B_kd_k + g_k$ (как обычно, через $g_k$ мы обозначаем $\nabla f(x_k)$). То, насколько мы сейчас близко к точке оптимума, показывает норма градиента $\|g_k\|$, поэтому в качестве требования на невязку решения СЛАУ возьмём следующее условие: $\|r_k\| \leq \eta_k\|g_k\|$. Чем ближе  мы к точке оптимума, тем точнее нам придётся решать систему. Последовательность $\eta_k$ называется \textit{форсирующей последовательностью}; о ней будет сказано ниже. 

Итак, мы хотим решать СЛАУ до тех пор, пока не получится удовлетворить условию $\|r_k\| \leq \eta_k\|g_k\|$. Но гарантируем ли мы, что найденное неточное решение действительно будет <<показывать>> в нужную сторону, т.е. будет ли оно направлением спуска? Давайте разберемся. Вспомним, как устроен метод сопряженных градиентов: решение СЛАУ $B_kd_k = -g_k$ эквивалентно минимизации
$$\varphi(d) = \frac{1}{2}d^TB_kd + d^Tg_k \to \min_d$$
при этом мы знаем, что с каждой итерацией значение $\varphi(d)$ неувеличивается (в силу частичной оптимальности). Запустим метод из начального приближения $d_0 = 0$; обозначим через $d_i$ частичное решение на $i$-ом шаге метода сопряженных градиентов. Тогда в силу вышесказанного, $\varphi(d_i) \leq \varphi(d_0) = 0$ и 
$$d_i^Tg_k = \varphi(d_i) - \frac{1}{2}d_i^TB_kd_i \leq - \frac{1}{2}d_i^TB_kd_i < 0$$
в силу положительной определенности матрицы $B_k$. Итак, мы получили, что если мы начнем искать $d_k$ из начальной точки $d_0 = 0$, на какой бы итерации мы не оборвали метод сопряженных градиентов, мы получим направление спуска.

\textbf{Локальная скорость сходимости HFN}. Оказывается, HFN так же, как и метод Ньютона, локально сходится квадратично. Покажем это. Пусть, как и раньше, $f \in C^{2,2}_M$, $\nabla^2 f(x_{\mathrm{opt}}) \geq \mu I$, $\alpha_k = 1$, $\|x_k - x_{\mathrm{opt}}\| \leq \mathrm{const}$. Докажем сначала пару вспомогательных утверждений.

\begin{enumerate}
    \item $\|g_k\| = O(\|x_k - x_{\mathrm{opt}}\|)$. Воспользуемся рядом Тейлора и разложим $\nabla f(x)$ в точке $x = x_{\mathrm{opt}}$:
    $$g_k = \nabla f(x_k) = \nabla f(x_{\mathrm{opt}}) + O(\|x_k - x_{\mathrm{opt}}\|)$$
    Но $\nabla f(x_{\mathrm{opt}})$, откуда немедленно следует доказываемое.
    
    \item $\|B_k(x_k - x_{\mathrm{opt}}) - g_k\| = O(\|x_k - x_{\mathrm{opt}}\|^2)$. Действительно, опять разложим $\nabla f(x)$ в ряд Тейлора в точке $x = x_{\mathrm{opt}}$:
    $$
        g_k = \nabla f(x_k) = \nabla f(x_{\mathrm{opt}}) + B_k(x_k - x_{\mathrm{opt}}) + O(\|x_k - x_{\mathrm{opt}}\|^2) 
    $$
    Но ведь $\nabla f(x_{\mathrm{opt}}) = 0$, а это значит, что $$g_k - B_k(x_k - x_{\mathrm{opt}}) = O(\|x_k - x_{\mathrm{opt}}\|^2)$$ а значит по определению $\|B_k(x_k - x_{\mathrm{opt}}) - g_k\| = O(\|x_k - x_{\mathrm{opt}}\|^2)$.
    
\end{enumerate}

Напомним, что $x_{k+1} = x_k + \alpha_k d_k = x_k + d_k$, $r_k = B_kd_k + g_k$, и мы требуем $\|r_k\| \leq \eta_k\|g_k\|$. Рассмотрим
\begin{align}
    \|x_{k+1} - x_{\mathrm{opt}}\| &= \|x_k - x_{\mathrm{opt}} + d_k\|  
    = \{r_k = B_kd_k + g_k\} = \|x_k - x_{\mathrm{opt}} + B_k^{-1}(r_k - g_k)\| \\ &= \|B_k^{-1}(B_k(x_k - x_{\mathrm{opt}}) + r_k - g_k)\| 
    \leq \|B_k^{-1}\| (\|B_k(x_k - x_{\mathrm{opt}}) - g_k\| + \|r_k\|) \\ &\leq 
    \{\|B_k^{-1}\| < \mathrm{const} \text{, см. лекцию 4}\} \leq \mathrm{const} \cdot (O(\|x_k - x_{\mathrm{opt}}\|^2) + \eta_k \|g_k\|) \\ &= \mathrm{const} \cdot (O(\|x_k - x_{\mathrm{opt}}\|^2) + \eta_k O(\|x_k - x_{\mathrm{opt}}\|))
\end{align}

Теперь рассмотрим три случая:
\begin{enumerate}
    \item $\eta_k \leq \eta < 1$. Тогда $$O(\|x_k - x_{\mathrm{opt}}\|^2) + \eta_k O(\|x_k - x_{\mathrm{opt}}\|) = O(\|x_k - x_{\mathrm{opt}}\|)$$ и мы можем гарантировать лишь линейую сходимость.
    \item $\eta_k \to 0$, $k \to \infty$. Тогда 
    $$O(\|x_k - x_{\mathrm{opt}}\|^2) + \eta_k O(\|x_k - x_{\mathrm{opt}}\|) = O(\|x_k - x_{\mathrm{opt}}\|^{1 + \varepsilon})$$
    и мы можем гарантировать сверхлинейную скорость сходимости. Типичным выбором является $\eta_k = \min\{1/2, \sqrt{\|g_k\|}\}$ (чтобы обеспечить $\eta_k < 1$). В этом случае мы получим <<сходимость 3/2>>, так как $\sqrt{\|g_k\|} = O(\|x_k - x_{\mathrm{opt}}\|^{1/2})$.
    \item $\eta_k = O(\|g_k\|) = O(\|x_k - x_{\mathrm{opt}}\|)$. Теперь получаем квадратичную сходимость:
    $$O(\|x_k - x_{\mathrm{opt}}\|^2) + \eta_k O(\|x_k - x_{\mathrm{opt}}\|) = O(\|x_k - x_{\mathrm{opt}}\|^2)$$
    Типичный выбор: $\eta_k = \min\{1/2, \|g_k\|\}$.
\end{enumerate}

Нельзя сказать, что второй подход хуже третьего. На практике часто бывает достаточно уже сверхлинейной сходимости, причём не обязательно третий метод будет работать быстрее: так как в третьем случае $\eta_k$ быстрее сходится к нулю, чем во втором случае, в третьем методе $\|r_k\|$ убывает быстрее, и мы раньше начинаем искать точные решения, что, разуемеется, негативно сказывается на времени работы программы.


% \begin{algorithm}[b]
% \caption{Hessian-Free Newton}\label{alg:HFN}
% \begin{algorithmic}[1]
% \State $g_0 \gets \nabla f(x_0)$
% \For{$k = 0, 1, 2, \dots $}
%     \State $\varepsilon_k \gets \min\{1/2, \sqrt{\|g_k\|}\} \cdot \|g_k\|$
%     \State $z_0 \gets 0$, $r_0 = g_k$, $p_0 \gets -r_0$
%     \For{$j = 0, 1, \dots, 2n-1$}
%         \If {p_j^TB_kp_j \leq \delta}
%     \EndFor
%     \State $x_{k+1} = x_k + \alpha_kd_k$
%     \State $g_{k+1} = Ax_{k+1} - b$
%     \If {$\|g_{k+1}\|^2 \leq \varepsilon$} \Then
%         \State\textbf{break}
%     \EndIf
%     \State $\beta_k = \frac{g_{k+1}^Tg_{k+1}}{g_k^Tg_k}$
%     \State $d_{k+1} = -g_{k+1} + \beta_kd_k$
% \EndFor
% \end{algorithmic}
% \end{algorithm}

\subsection{Умножение гессиана на вектор}
Теперь мы рассмотрим, как же можно умножить гессиан на вектор, не вычисляя самого гессиана. Но для начала обратимся к примеру логистической регрессии. Известно, что гессиан её функции потерь выражается как
$$\nabla^2 f(w) = \frac{1}{N} X^TSX + \lambda I$$
где $X \in \mathbb{R}^{N \times D}$ --- матрица объект-признак, $S = \diag(s_1, \dots, s_N)$, $s_i = \sigma(y_iw^Tx_i)(1 - \sigma(y_iw^Tx_i))$, а $\lambda$ --- коэффициент $L$2-регуляризации.

Мы хотим вычислить $\nabla^2 f(w) d$:
$$u = \nabla^2 f(w) d = \frac{1}{N}X^T\underbrace{S\overbrace{Xd}^{z}}_{y} + \lambda d$$
Тогда $z = Xd$ можно вычислить за $O(ND)$, $y = Sz$ --- за $O(N)$, а $u = \frac{1}{N}X^Ty + \lambda d$ --- за $O(ND)$. В итоге, мы посчитали произведение гессиана на вектор за $O(ND)$. Если бы мы напрямую считали гессиан, мы бы потратили $O(DND)$ операций на его вычисление и $O(D^2)$ на нахождение произведения гессиана и вектора; в итоге сложность получилась бы $O(ND^2)$, что очевидно хуже, чем $O(ND)$. Итак, первая идея: можно использовать структуру гессиана для его эффективного умножения на вектор.

\begin{figure*}[t]
    \centering
    \includegraphics[width=.7\textwidth]{magic_mult}
    \caption{Зависимость невязки от $\varepsilon$ для различных подходов. График нарисован в логарифмических шкалах. В качестве функции была взята $f(x) = \frac{1}{2} \|xx^T + A\|_F^2$, $A \in \mathbb{S}^n$}\label{gr:mm}
\end{figure*}

Вторая идея: можно оценить значение $\nabla^2 f(x) d$. Рассмотрим три возможных подхода.
\begin{enumerate}
    \item Разложим $\nabla f(x + \varepsilon d)$ в ряд Тейлора с центром в точке $x$:
    $$\nabla f(x + \varepsilon d) = \nabla f(x) + \varepsilon \nabla^2 f(x) d + O(\varepsilon^2)$$
    откуда получим, что
    $$\nabla^2 f(x) = \frac{f(x + \varepsilon d) - f(x)}{\varepsilon} + O(\varepsilon) \Rightarrow \nabla^2 f(x) \approx \frac{f(x + \varepsilon d) - f(x)}{\varepsilon}$$
    Обозначим полученную оценку за $\Theta_1$, а настоящее значение $\nabla^2 f(x) d$ за $\Theta_{\mathrm{true}}$. На рисунке \ref{gr:mm} показана зависимость $\|\Theta_1 - \Theta_{\mathrm{true}}\|$ от $\varepsilon$. Оказывается, значение $\varepsilon$, при котором невязка минимальна, равно корню из машинной точности $\sqrt{\varepsilon_m}$. При уменьшении $\varepsilon$ в игру вступают погрешности арифметики чисел с плавающей точкой, и невязка начинает расти.
    
    \item Оказывается, что справедливо следующее соотношение:
    $$
        \nabla^2 f(x) d = \underbrace{\frac{\nabla f(x + \varepsilon d) - \nabla f(x - \varepsilon d)}{2\epsilon}}_{\Theta_2} + O(\varepsilon^2)
    $$
    На рисунке \ref{gr:mm} изображена зависимость $\|\Theta_2 - \Theta_{\mathrm{true}}\|$ от $\varepsilon$ в логарифмических шкалах. Значение $\varepsilon$, при котором невязка минимальна, равно $\sqrt[3]{\varepsilon_m}$ (что, кстати, хорошо видно на графике). Отметим, что если в первом методе нам требовалось найти градиент в одной дополнительной точке, сейчас нам придется считать его в двух дополнительных точках, что более затратно по времени.
    
    \item Следующий метод работает для бесконечно дифференцируемых функций $f(x)$. Рассмотрим функцию $\hat{f}: \mathbb{C}^n \to \mathbb{C}$ такую, что $\forall x \in \mathbb{R}^n$ выполняется $\hat{f}(x) = f(x)$ (то есть мы расширяем функцию из вещественного пространства в комплексное). Оказывается, что мы можем записать ряд Тейлора для $\hat{f}(x)$ до члена любого порядка, но нам хватит третьего порядка:
    $$\nabla \hat{f}(x + i\varepsilon d) = \nabla f(x) + \nabla^2 f(x) i\varepsilon d + O(\varepsilon^2) + iO(\varepsilon^3)$$
    Откуда $$\Im{\left[\nabla \hat{f}(x + i\varepsilon d)\right]} = \nabla^2 f(x) \varepsilon d + O(\varepsilon^3)$$
    А значит
    $$\nabla^2 f(x) d = \frac{\Im{\left[\nabla \hat{f}(x + i\varepsilon d)\right]}}{\varepsilon} + O(\varepsilon^2)$$
    При программировании на Python нам не нужно будет реализовывать никакие расширения функций, а просто подать на вход функции \textsf{f\(\)} комплексный вектор:\\ \textsf{f(x + epsilon * d * 1j)}, так как в Python встроена работа с комплексными числами. Теперь заметим, что в числителе дроби мы не вычитаем близкие друг к другу значения, как в предыдущих примерах! Мы просто делим на $\varepsilon$, а погрешностей при умножении и делении чисел с плавающей точкой практически нет. На рисунке \ref{gr:mm} видно, что с уменьшением $\varepsilon$ невязка не начинает расти, как в предыдущих двух случаях, а выходит на константу при $\varepsilon > \sqrt{\varepsilon_m}$. Итак, теперь мы умеем практически без погрешностей находить произведение гессиана на произвольный вектор (главное, чтобы $f$ была бесконечно дифференцируемой).
\end{enumerate}



\end{document}
