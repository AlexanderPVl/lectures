\section{Лекция 10 от 12.05.2017}
\subsection{Доверительные области}
Наш курс покрывает три основных метода статистики~--- точечные оценки, доверительные 
интервалы и проверка гипотез. Но точечные оценки, в некоторой степени, являются 
экзотикой. Продолжим рассматривать доверительные интервалы.

Ранее мы вводили их для одномерного случая. Теперь вопрос: а можно ли обобщить это на 
многомерный параметр? Можно. В таком случае строят так называемую \emph{доверительную 
область}. Определяется она аналогично доверительному интервалу. Поймём, как её строить.

\begin{problem}
    Пусть \((X_{1}, \dots, X_{n})\)~--- выборка из нормального распределения 
    \(\mathcal{N}(\theta_{1}, \theta_{2}^{2})\). Постройте доверительную область для 
    \(\theta = (\theta_{1}, \theta_{2}^{2})\).
\end{problem}

Для начала решим две подзадачи:
\begin{enumerate}
    \item Как построить доверительный интервал для \(\theta_{2}^{2}\) при неизвестном 
    \(\theta_{1}\)?
    \item Как построить доверительный интервал для \(\theta_{1}\) при неизвестном 
    \(\theta_{2}^{2}\)?
\end{enumerate}

Начнём с первой подзадачи. Для начала вспомним, что для \(\theta_{2}^{2}\) есть 
несмещённая оценка
\[
    S^{2} = \frac{1}{n - 1}\sum_{k = 1}^{n} (X_{k} - \overline{X})^{2}.
\]
Тогда можно рассмотреть центральную статистику вида \(S^{2}/\theta_{2}^{2}\). Что мы 
можем про неё сказать? На самом деле, много чего. Например, то, что она не зависит от 
\(\theta\). 

\begin{theorem}
    Пусть \(X_{1}, \dots, X_{n}\)~--- выборка из \(\mathcal{N}(\theta_{1}, 
    \theta_{2}^{2})\), а \(S^{2}\)~--- несмещённая выборочная дисперсия. Тогда
    \[
        (n - 1)\frac{S^{2}}{\theta_{2}^{2}} \sim \chi^{2}_{n - 1}.
    \]
\end{theorem}

Но для доказательства этого факта нужно ещё несколько утверждений.
\begin{theorem}
    Пусть \(X_{1}, \dots, X_{n}\)~--- выборка из \(\mathcal{N}(\theta_{1}, 
    \theta_{2}^{2})\). Тогда выборочные среднее \(\overline{X}\) и несмещённая дисперсия 
    \(S^{2}\) независимы.
\end{theorem}

Казалось бы: в построении обеих случайных величин участвуют все \(X_{1}, \dots, X_{n}\), 
причём в достаточно похожей манере. Но всё равно получается независимость. Приступим.

\begin{proof}
    Докажем следующее свойство: \(\overline{X}\) независимо с \(X_{i} - \overline{X}\) 
    для любого \(i \in \{1, \dots, n\}\). Для начала покажем, что они некоррелированны:
    \begin{align*}
        \cov(\overline{X}, X_{i} - \overline{X}) &= \cov(\overline{X}, X_{i}) - 
        \cov(\overline{X}, \overline{X}) = \frac{1}{n}\sum_{k = 1}^{n}\cov(X_{k}, X_{i}) 
        - \D{\overline{X}} = \\
        &= \frac{1}{n}\cov(X_{i}, X_{i}) - \frac{\theta_{2}^{2}}{n} = 0.
    \end{align*}
    
    Теперь заметим, что \((\overline{X}, X_{i} - \overline{X})\) является гауссовким 
    вектором, так как его можно получить линейными преобразованиями следующего типа 
    (покажем на 
    примере \(X_{1}\)):
    \[
    \begin{pmatrix}
    \overline{X} \\ X_{1} - \overline{X}
    \end{pmatrix}
    =
    \begin{pmatrix}
    \frac{1}{n} & \frac{1}{n} & \frac{1}{n} & \ldots & \frac{1}{n} \\
    \frac{n - 1}{n} & -\frac{1}{n} & -\frac{1}{n} & \ldots & -\frac{1}{n}
    \end{pmatrix}
    \begin{pmatrix}
    X_{1} \\ X_{2} \\ \vdots \\ X_{n}
    \end{pmatrix}
    \] 
    
    Из этого следует независимость. Теперь заметим, что \(S^{2}\) есть функция от \(X_{i} 
    - \overline{X}\), \(i \in \{1, 2, \dots, n\}\). Так как все они независимы с 
    \(\overline{X}\), то и \(S^{2}\) независимо с \(X\).
\end{proof}

\begin{lemma}
    Пусть \(X_{1}, \dots, X_{n}\)~--- случайные величины со средним \(\mu\). Тогда
    \[
        \sum_{k = 1}^{n} (X_{k} - \mu)^{2} = \sum_{k = 1}^{n} (X_{k} - \overline{X})^{2} 
        + n(\overline{X} - \mu)^{2}.
    \]
\end{lemma}
\begin{proof}
    Распишем сумму слева:
    \begin{align*}
        \sum_{k = 1}^{n} (X_{k} - \mu)^{2} &= \sum_{k = 1}^{n} (X_{k}^{2} - 2\mu X_{k} + 
        \mu^{2}) = \sum_{k = 1}^{n} X_{k}^{2} - 2\mu\sum_{k = 1}^{n} X_{k} + n\mu^{2} = 
        \\ &= \sum_{k = 1}^{n} X_{k}^{2} - 2n\mu\overline{X} + n\mu^{2}.
    \end{align*}
    Теперь заметим, что
    \begin{align*}
        (n - 1)S^{2} &= \sum_{k = 1}^{n} (X_{k} - \overline{X})^{2} = \sum_{k = 1}^{n} 
        \left(X_{k}^{2} - 2X_{k}\overline{X} + \overline{X}^{2}\right) = \\ 
        &= \sum_{k = 1}^{n} X_{k}^{2} + n\overline{X}^{2} - 2\overline{X}\sum_{k = 
        1}^{n}X_{k} = \sum_{k = 1}^{n} X_{k}^{2} - n\overline{X}^{2}.
    \end{align*}
    Тогда
    \[
        \sum_{k = 1}^{n} (X_{k} - \mu)^{2} = (n - 1)S^{2} + n\overline{X}^{2} - 
        2n\mu\overline{X} + n\mu^{2} = \sum_{k = 1}^{n} (X_{k} - \overline{X})^{2} 
        + n(\overline{X} - \mu)^{2}. \qedhere
    \]
\end{proof}

\begin{lemma}
    Пусть \(X_{1}, \dots, X_{n}\)~--- независимые и одинаково распределённые случайные 
    величины с стандартным нормальным распределением \(\mathcal{N}(0, 1)\). Тогда
    \[
        X_{1}^{2} + \dots + X_{n}^{2} \sim \chi^{2}_{n}.
    \]
\end{lemma}
\begin{proof}
    Для доказательства этого факта нужно показать, что \(X_{1}^{2} \sim \chi^{2}_{1}\) и 
    \(\chi^{2}_{a} + \chi^{2}_{b} \sim \chi^{2}_{a + b}\). Начнём с первого. Рассмотрим 
    функцию распределения \(X_{1}^{2}\) в точке \(z \geq 0\):
    \[
        F_{X_{1}^{2}}(z) = \Pr{X_{1}^{2} \leq z} = \Pr{-\sqrt{z} \leq X_{1} \leq 
        \sqrt{z}} = \Pr{X_{1} \leq \sqrt{z}} - \Pr{X_{1} \leq -\sqrt{z}}.
    \]
    Тогда
    \[
        p_{X_{1}^{2}}(z) = \frac{1}{2\sqrt{z}}p_{X_{1}}(\sqrt{z}) + 
        \frac{1}{2\sqrt{z}}p_{X_{1}}(-\sqrt{z}).
    \]
    Так как \(\mathcal{N}(0, 1)\)~--- симметричное относительно нуля распределение, то
    \[
        p_{X_{1}^{2}}(z) = \frac{1}{\sqrt{z}}p_{X_{1}}(\sqrt{z}) = 
        \frac{z^{-\frac{1}{2}}e^{-\frac{z}{2}}}{\sqrt{2\pi}} = 
        \frac{2^{-\frac{1}{2}}z^{-\frac{1}{2}}e^{-\frac{z}{2}}} 
        {\Gamma\left(\frac{1}{2}\right)} = p_{\chi^{2}_{1}}(z) \implies X_{1}^{2} \sim 
        \chi^{2}_{1}.
    \]
    
    Теперь покажем, что \(\chi^{2}_{a} + \chi^{2}_{b} \sim \chi^{2}_{a + b}\). По формуле 
    свёртки плотность \(\chi^{2}_{a} + \chi^{2}_{b}\) в точке \(z > 0\) равна
    \begin{align*}
        p_{\chi^{2}_{a} + \chi^{2}_{b}}(z) &= \int\limits_{-\infty}^{+\infty} 
        p_{\chi^{2}_{a}}(z - x)p_{\chi^{2}_{b}}(x)\diff x = \\ 
        &= \int\limits_{-\infty}^{+\infty} \frac{2^{-\frac{a}{2}} (z - x)^{\frac{a}{2} - 
        1} e^{-\frac{z - x}{2}}}{\Gamma\left(\frac{a}{2}\right)} \frac{2^{-\frac{b}{2}} 
        x^{\frac{b}{2} - 1} e^{-\frac{x}{2}}}{\Gamma\left(\frac{b}{2}\right)} \I\{x > 0\} 
        \I\{z - x > 0\} \diff x = \\
        &= \frac{2^{-\frac{a + b}{2}}e^{-\frac{z}{2}}}{\Gamma\left(\frac{a}{2}\right) 
        \Gamma\left(\frac{b}{2}\right)} \int\limits_{0}^{z} (z - x)^{\frac{a}{2} - 
        1}x^{\frac{b}{2} - 1}\diff x = \\
        &= \frac{2^{-\frac{a + b}{2}}e^{-\frac{z}{2}}z^{-\frac{a + b}{2} - 
        1}}{\Gamma\left(\frac{a}{2}\right) \Gamma\left(\frac{b}{2}\right)} 
        \int\limits_{0}^{1} \left(1 - \frac{x}{z}\right)^{\frac{a}{2} - 
        1}\left(\frac{x}{z}\right)^{\frac{b}{2} - 1}\diff \left(\frac{x}{z}\right) = \\
        &= \frac{2^{-\frac{a + b}{2}}e^{-\frac{z}{2}}z^{-\frac{a + b}{2} - 
        1}}{\Gamma\left(\frac{a}{2}\right) \Gamma\left(\frac{b}{2}\right)} 
        \mathrm{B}\left(\frac{b}{2}, \frac{a}{2}\right) = \frac{2^{-\frac{a + 
        b}{2}}e^{-\frac{z}{2}}z^{-\frac{a + b}{2} - 1}}{\Gamma\left(\frac{a + 
        b}{2}\right)} = p_{\chi^{2}_{a + b}}(z).
    \end{align*}
    
    Из этих двух свойств сразу же получаем желаемое.
\end{proof}
\begin{remark}
    Вообще говоря, именно из этого рассуждения и появилось распределение хи-квадрат. И 
    поэтому его часто вводят, как распределение суммы квадратов стандартных нормальных 
    случайных величин.
\end{remark}

\begin{lemma}
    Пусть случайная величина \(\xi\) имеет распределение \(\chi^{2}_{n}\). Тогда 
    характеристическая функция этой случайной величины равна \((1 - 2it)^{-\frac{n}{2}}\).
\end{lemma}
\begin{proof}[Решение]
    Будем считать по определению:
    \[
        \phi_{\xi}(t) = \E{e^{it\xi}} = \int\limits_{0}^{+\infty} 
        \frac{2^{-\frac{n}{2}}e^{-\frac{x}{2} + itx}x^{-\frac{n}{2} - 
        1}}{\Gamma\left(\frac{n}{2}\right)}\diff x.
    \]
    
    Сделаем замену \(u = (1 - 2it)x\). Тогда интеграл равен
    \[
        \phi_{\xi}(t) = \frac{1}{(1 - 2it)^{\frac{n}{2}}}\int\limits_{0}^{+\infty} 
        \frac{2^{-\frac{n}{2}}e^{-\frac{u}{2}}u^{-\frac{n}{2} - 
        1}}{\Gamma\left(\frac{n}{2}\right)}\diff u = \frac{1}{(1 - 2it)^{\frac{n}{2}}}. 
        \qedhere
    \]
\end{proof}

Финальный рывок~--- само доказательство.
\begin{proof}
    Согласно лемме о сумме квадратов центрированных случайных величин
    \[
        \sum_{k = 1}^{n} \left(\frac{X_{k} - \theta_{1}}{\theta_{2}}\right)^{2} = 
        \frac{1}{\theta_{2}^{2}}\sum_{k = 1}^{n} (X_{k} - \overline{X})^{2} 
        + n\left(\frac{\overline{X} - \theta_{1}}{\theta_{2}}\right)^{2} = \frac{(n - 
        1)S^{2}}{\theta_{2}^{2}} + \left(\frac{\overline{X} - 
        \theta_{1}}{\theta_{2}/\sqrt{n}}\right)^{2}.
    \]
    
    Теперь заметим, что члены правой суммы независимы, как функции от независимых 
    \(\overline{X}\) и \(S^{2}\). Тогда имеет смысл смотреть на характеристические 
    функции.
    
    Слева стоит сумма квадратов нормированных и центрированных нормальных случайных 
    величин. Как известно, она имеет распределение \(\chi^{2}_{n}\). Самый правый же член 
    имеет распределение \(\chi^{2}_{1}\), так как дисперсия \(\overline{X}\) равна 
    \(\theta_{2}^{2}/n\). Тогда, обозначив характеристическую функцию \((n - 
    1)S^{2}/\theta_{2}^{2}\) за \(\phi\), получаем, что
    \[
        (1 - 2it)^{-\frac{n}{2}} = \phi(t)(1 - 2it)^{-\frac{1}{2}} \implies \phi(t) = (1 
        - 2it)^{-\frac{n - 1}{2}}.
    \]
    
    Но это есть ни что иное, как характеристическая функция \(\chi^{2}_{n - 1}\). Тогда 
    \((n - 1)S^{2}/\theta_{2}^{2} \sim \chi^{2}_{n - 1}\), что и требовалось доказать.
\end{proof}

Великолепно. Тогда можно сказать, что доверительный интервал должен иметь вид 
\((c_{1}S^{2}, c_{2}S^{2})\). Для простоты скажем, что он центральный, то есть выполнено 
следующее условие:
\[
    \Pr{\theta_{2}^{2} \geq c_{2}S^{2}} = \Pr{\theta_{2}^{2} \leq c_{1}S^{2}} = \frac{1 
    - \gamma}{2}.
\]
Отсюда уже несложно получить интервал. Преобразуем эти равенства:
\[
    \Pr{(n - 1)\frac{S^{2}}{\theta_{2}^{2}} \leq \frac{n - 1}{c_{2}}} = \frac{1 - 
    \gamma}{2}, \quad \Pr{(n - 1)\frac{S^{2}}{\theta_{2}^{2}} \leq \frac{n - 1}{c_{1}}} = 
    \frac{1 + \gamma}{2}.
\]
Из этих уравнений получаем, что \(c_{1} = (n - 1)/t_{1}\), где \(t_{1}\)~--- это решение 
уравнения
\[
    \int\limits_{0}^{t_{1}} \frac{2^{-\frac{n - 1}{2}}e^{-\frac{z}{2}}z^{-\frac{n - 1}{2} 
    - 1}}{\Gamma\left(\frac{n - 1}{2}\right)}\diff z = \frac{1 - \gamma}{2} \implies 
    t_{1} = F_{\chi^{2}_{n - 1}}^{-1}\left(\frac{1 - \gamma}{2}\right).
\]

Аналогично со вторым концом. 

Теперь разберёмся с интервалом для матожидания. Но тут есть один серьёзный подводный 
камень:
\[
    \overline{X} - \theta_{1} \sim \mathcal{N}\left(0, \frac{\theta_{2}^{2}}{n}\right). 
\]

Нужно как-то избавиться от \(\theta_{2}^{2}\). На помощь нам придёт несмещённая 
выборочная дисперсия:
\[
    \frac{\sqrt{n}(\overline{X} - \theta_{1})}{\theta_{2}} = \frac{S}{\theta_{2}} 
    \frac{\sqrt{n}(\overline{X} - \theta_{1})}{S} \sim \mathcal{N}(0, 1)
\]

Теперь возникает следующий вопрос: а какое распределение у каждой из дробей? Если первую 
ещё можно понять, то со второй вообще всё печально. Разберёмся.

\begin{problem}
    Пусть \(X_{1}, \dots, X_{n}\)~--- независимые и одинаково распределённые случайные 
    величины из распределения \(\mathcal{N}(\mu, \sigma^{2})\). Найдите распределение 
    случайной величины
    \[
        \sqrt{n}\frac{\overline{X} - \mu}{S}, \text{ где } S^{2} = \frac{1}{n - 1}\sum_{k 
        = 1}^{n}(X_{k} - \overline{X})^{2}.
    \]
\end{problem}
\begin{proof}[Решение]
    Воспользуемся ранее доказанным фактом о том, что \(\overline{X}\) и \(S^{2}\) 
    независимы. Далее заметим, что \(\sqrt{n}(\overline{X} - \mu) \sim \mathcal{N}(0, 
    \sigma^{2})\). Тогда, введя случайную величину \(Y = \sqrt{n}(\overline{X} - 
    \mu)/\sigma \sim \mathcal{N}(0, 1)\), получаем, что
    \[
        t = \sqrt{n}\frac{\overline{X} - \mu}{S} = \frac{\sigma Y}{S} = \sqrt{n - 1} 
        \frac{Y}{\sqrt{n - 1}\frac{S}{\sigma}}.
    \]
    
    Теперь возведём в квадрат и сразу поделим на \(n - 1\)
    \[
        \frac{t^{2}}{n - 1} = \frac{Y^{2}}{(n - 1)S^{2}/\sigma^{2}} \sim 
        \frac{\chi^{2}_{1}}{\chi^{2}_{n - 1}}.
    \]
    
    Теперь нужно посчитать плотность частного. По формуле свёртки:
    \begin{align*}
        p_{\chi^{2}_{1}/\chi^{2}_{n}}(x) &= \int\limits_{-\infty}^{+\infty} 
        |y|p_{\chi^{2}_{1}}(xy)p_{\chi^{2}_{n}}(y)\diff y = \\
        &= \int\limits_{0}^{\infty} y\frac{2^{-\frac{1}{2}}(xy)^{-\frac{1}{2}} 
        e^{-\frac{xy}{2}}}{\Gamma\left(\frac{1}{2}\right)}\frac{2^{-\frac{n - 1}{2}} 
        y^{\frac{n - 1}{2} - 1} e^{-\frac{y}{2}}}{\Gamma\left(\frac{n - 
        1}{2}\right)}\diff y = \\
        &= \frac{2^{-\frac{n}{2}}x^{-\frac{1}{2}}}{\Gamma\left(\frac{1}{2}\right) 
        \Gamma\left(\frac{n - 1}{2}\right)} \int\limits_{0}^{\infty} y^{\frac{n}{2} - 
        1}\exp\left\{-y\frac{x + 1}{2}\right\}\diff y
    \end{align*}
    
    Сделаем замену \(t = (x + 1)y/2\). Тогда
    \[
        p_{\chi^{2}_{1}/\chi^{2}_{n}}(x) = \frac{x^{-\frac{1}{2}}}{(1 + 
        x)^{\frac{n}{2}}\Gamma\left(\frac{1}{2}\right) \Gamma\left(\frac{n - 
        1}{2}\right)} \int\limits_{0}^{\infty} t^{\frac{n}{2} - 1}e^{-t}\diff t = 
        \frac{x^{-\frac{1}{2}}}{(1 + x)^{\frac{n}{2}}} 
        \frac{\Gamma\left(\frac{n}{2}\right)} {\Gamma\left(\frac{1}{2}\right) 
        \Gamma\left(\frac{n - 1}{2}\right)}.
    \]
    
    Теперь посчитаем распределение \(t\). Для этого посмотрим на функцию распределения 
    модуля:
    \[
        F_{|t|}(x) = \Pr{|t| \leq x} = \Pr{\frac{t^{2}}{n - 1} \leq \frac{x^{2}}{n - 1}}
    \]
    
    Тогда
    \[
        p_{|t|}(x) = \frac{2x}{n - 1}p_{\chi^{2}_{1}/\chi^{2}_{n}}\left(\frac{x^{2}}{n - 
        1}\right) = \frac{2x}{n - 1}\frac{\sqrt{n - 1}}{(1 + \frac{x^{2}}{n - 
        1})^{\frac{n}{2}}x}\frac{\Gamma\left(\frac{n}{2}\right)} 
        {\Gamma\left(\frac{1}{2}\right) \Gamma\left(\frac{n - 1}{2}\right)}
    \]
    
    Теперь заметим, что у \(t\) симметричное относительно нуля распределение. Тогда
    \begin{align*}
        p_{t}(x) = \frac{1}{2}p_{|t|}(x) &= \frac{\Gamma(n/2)}{\sqrt{\pi(n - 1)}\Gamma((n 
        - 1)/2)}\left(1 + \frac{x^{2}}{n - 1}\right)^{-\frac{n}{2}} = \\ &= 
        \frac{1}{\sqrt{n - 1}\,\mathrm{B}\left(\frac{1}{2}, \frac{n - 
        1}{2}\right)}\left(1 + \frac{x^{2}}{n - 1}\right)^{-\frac{n}{2}}. \qedhere
    \end{align*}
\end{proof}
Полученное распределение называют \emph{распределением Стьюдента} с \(n - 1\) степенью 
свободы и обозначают \(t(n - 1)\).

\begin{remark}
    При вычислении распределения мы смотрели на распределение дроби 
    \(\chi^{2}_{1}/\chi^{2}_{n - 1}\). С этим напрямую связано так называемое 
    \emph{распределение Фишера}, которое является распределением дроби 
    \[
        \xi = \frac{d_{2}\chi^{2}_{d_1}}{d_{1}\chi^{2}_{d_2}}.
    \]
\end{remark}

Теперь можно сказать, что есть смысл искать интервал в виде \((\overline{X} - \epsilon, 
\overline{X} + \epsilon)\). Тогда
\[
    \gamma = \Pr{\overline{X} - \epsilon \leq \theta_{1} \leq \overline{X} + \epsilon} = 
    \Pr{-\frac{\epsilon\sqrt{n}}{S} \leq \sqrt{n}\frac{\overline{X} - \mu}{S} \leq 
    \frac{\epsilon\sqrt{n}}{S}}.
\]

Отсюда несложно получить, что
\[
    \epsilon = \frac{S\mu}{\sqrt{n}}, \text{ где } \int\limits_{-\mu}^{\mu} 
    \frac{1}{\sqrt{n - 1}\,\mathrm{B}\left(\frac{1}{2}, \frac{n - 1}{2}\right)}\left(1 + 
    \frac{x^{2}}{n - 1}\right)^{-\frac{n}{2}}\diff x = \gamma.
\]

Теперь задача: у нас есть доверительные интервалы. Как их объединить в доверительную 
область? Для этого посмотрим на определение доверительной области:
\[
    \Pr{\theta \in (T_{1}(X_{1}, \dots, X_{n}), T_{2}(X_{1}, \dots, X_{n})} \geq \gamma.
\]

В нашем случае оно будет иметь следующий вид:
\[
    \Pr{\theta_{1} \in (\overline{X} - \epsilon, \overline{X} + \epsilon), \theta_{2} \in 
    (c_{1}S^{2}, c_{2}S^{2})} \geq \gamma.
\]

Теперь вспомним формулу включений-исключений и сделаем оценку снизу:
\[
    \Pr{A \cap B} = \Pr{A} + \Pr{B} - \Pr{A \cup B} \geq \Pr{A} + \Pr{B} - 1.
\]

Тогда, подгоняя значения для каждого интервала, мы сможем получить желаемый коэффициент 
доверия.

\subsection{Проверка гипотез. Ошибки первого и второго рода}
Теперь мы перешли к третьему и самому полезному для практики разделу~--- проверка 
гипотез. 

Как мы все помним, математическая статистика кружится вокруг определения распределения по 
данным. Пусть есть статистическая структура \((\X, \F, \mathcal{P})\) и взята выборка 
\((X_{1}, \dots, X_{n})\) из распределения \(\mathcal{L}(X) \in \mathcal{P}\). Но мы 
ничего не знаем о том, какое у нас распределение, а можем лишь предполагать что-то о его 
структуре. В этом и есть суть гипотезы. Обычно вводят так называемую нулевую гипотезу и 
её альтернативу. Дадим формальное определение:
\begin{definition}
    Пусть \(\mathcal{P}_{0}\) и \(\mathcal{P}_{1}\)~--- непересекающиеся собственные 
    подмножества \(\mathcal{P}\). Тогда \emph{нулевой гипотезой} называется предположение 
    \(H_{0} : \mathcal{L}(x) \in \mathcal{P}_{0}\), а \emph{альтернативной} называется 
    предположение \(H_{1} : \mathcal{L}(x) \in \mathcal{P}_{1}\).
\end{definition}
Заметим, что в общем случае \(\mathcal{P}_{0} \cup \mathcal{P}_{1} \neq \mathcal{P}\).

В данном случае общий вопрос имеет такой вид: пусть есть выборка 
\((X_{1}, \dots, X_{n})\) из распределения \(\mathcal{L}(X) \in \mathcal{P}\). <<Верна>> 
ли гипотеза \(H_{0}\)? Но говорить, что гипотеза верна, не совсем верно (каламбур-с). 
Надо говорить <<нет причин отвергнуть гипотезу \(H_{0}\)>>. Но эта фраза слишком длинная, 
поэтому говорят <<верна>>, а понимают это~--- получается эдакий статистический слэнг.

Далее, гипотезы можно разбить на два класса: \emph{простые} и \emph{сложные}. Простыми 
гипотезами считаются гипотезы, в которых явно задаётся распределение. Например: \(H_{0} : 
\mathcal{L}(x) = \mathcal{N}(0, 1)\). Если же распределений больше одного, то гипотеза 
считается сложной.

То, что мы выбрали какую-то гипотезу, ещё не значит, что она верна. Может оказаться, что 
мы <<отвергли верную>> или же <<приняли ложную>> гипотезу. Но так говорить не совсем 
корректно. Обычно для этого вводится так называемая матрица ошибок:
\begin{center}
    \begin{tabular}{|c|c|c|c|}
        \hline \diaghead{*****************}{Предположение}{Реальность} & 
        \(H_{0}\) & \(H_{1}\) \\
        \hline&&\\[-10pt]
        \(H_{0}\) & \(H_{0}\) верно принята & \(H_{0}\) неверно принята
        \\[4pt]
        \hline&&\\[-10pt]
        \(H_{1}\) & \(H_{0}\) неверно отвергнута & \(H_{0}\) верно отвергнута \\
        \hline
    \end{tabular}
\end{center}

Ситуацию, когда вместо \(H_{0}\) принимается \(H_{1}\), называется \emph{ошибкой первого 
рода}. Аналогично, если вместо \(H_{1}\) принимается \(H_{0}\), называют \emph{ошибкой 
второго рода}.

Но за бортом остался главный вопрос: а как эти гипотезы строить? Для этого строятся так 
называемые критерии. Дадим определение:
\begin{definition}
    \emph{Критерием} называется правило, по которому по выборке делается заключение о 
    верности гипотезы.
\end{definition}