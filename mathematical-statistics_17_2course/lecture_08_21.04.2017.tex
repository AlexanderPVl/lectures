\section{Лекция 8 от 21.04.2017}
\subsection{Методы нахождения доверительных интервалов}

Мы уже поняли, доверительным интервалам в реальных задачах можно либо верить, либо нет --- сказать наверняка, повезло ли нам с выборкой, не получится.
Даже при высоком коэффициенте доверия есть вероятность, что мы ошиблись.

Снова пусть имеется параметр $\theta \in \Theta$ и повторная выборка $Y = (X_1, \ldots, X_n)$ из параметрического семейства $\mathcal{L}(x) \in \{\Pr_\theta, \theta \in \Theta\}.$
Имея реализацию выборки, мы хотим узнать не оценку неизвестного параметра, а некое множество такое, чтобы оно содержало $\theta$ с некоторой вероятностью, близкой к единице. 
Эта вероятность называется коэффициентом доверия (или коэффициентом надежности). 

В одномерном случае доверительное множество желательно искать в виде некоторого интервала.
При этом мы заметили, что чем меньше длина интервала (при фиксированном коэффициенте доверия), тем этот интервал информативнее.
Проблема возникает в том, что при построении доверительных интервалов, может оказаться, что не только концы интервала случайны, но и его длина.
Тогда возникает вопрос: если длина доверительного интервала --- случайная величина, то в каком смысле можно минимизировать ее длину?
Один из ответов --- можно минимизировать среднюю длину. Здесь возникает минимизация функции с некоторыми наложенными на нее условиями, то есть поиск условных экстремумов.

Пусть теперь $\Theta \subset \R^1$. Обозначим $p(\theta) = \Pr\left(\theta \in (T_1(Y),\ T_2(Y))\right)$, где $T_1, T_2$ --- некоторые статистики.
Тогда $\gamma = \inf_{\theta \in \Theta} p(\theta)$ есть коэффициент доверия, универсальный для всех возможных значений $\theta$.

Рассмотрим методы нахождения $T_1$ и $T_2$.
Сразу стоит обратить внимания на то, что все рассматриваемые нами методы требуют выполнения тех или иных условий, что стандартно для задач математической статистики.
Однако в большинстве реальных задач эти условия выполнены.

\subsection{Точечные оценки}
Пусть $F(z, \theta)$ --- функция распределения для распределения $\Pr_\theta$, то есть $F(z, \theta) = \Pr(X < z)$.
Предположим, что $F$ непрерывна и найдем распределение $F(X)$. Для $z \in (0, 1)$ имеем
\[
    \Pr(F(X, \theta) < z) = \Pr(X < F^{-1}(z)) = F(F^{-1}(z)) = z.
\]
Тогда получаем
\[
    \Pr(F(X, \theta) < z) =
    \begin{cases}
        0, & z \leqslant 0,\\
        1, & z \geqslant 1,\\
        z, & z \in (0, 1);
    \end{cases}
    \quad \Rightarrow \quad
    F(X, \theta) \sim U[0, 1],
\]
то есть $F(X, \theta)$ имеет равномерное на отрезке $[0, 1]$ распределение.
Таким образом, если мы хотим сгенерировать случайный набор данных с функцией распределения $F(z)$, достаточно уметь генерировать данные из распределения $U[0, 1]$ (а большинство статистических пакетов умеют это делать) и вычислять $F^{-1}$.

Вернемся к задаче доверительных интервалов.
Пусть $\theta^*$ --- точечная оценка для $\theta$, имеющая функцию распределения $F$.
Фиксируем $\epsilon \in (\frac12, 1)$ и найдем $\theta_1^*, \theta_2^*$ --- решения уравнений (относительно $\theta$)
\[
    F(\theta^*, \theta) = \epsilon, \qquad F(\theta^*, \theta) = 1 - \varepsilon
\]
соответственно. Предположим, что для всех значений $\theta^*$ значения $\theta_1^*$ и $\theta_2^*$ дают интервал $(\theta_1^*, \theta_2^*)$.
Тогда вероятность, что $\theta$ накрывается этим интервалом, есть
\[
    \Pr(\theta \in (\theta_1^*, \theta_2^*)) = 2\epsilon - 1.
\]

Для того, чтобы увидеть, как реализуется этот общий метод, рассмотрим классический пример.

\begin{example}
    Пусть выборка взята из нормального распределения с неизвестным средним: $Y = (X_1, \ldots, X_n) \in \mathcal{L}(X)$, где $X \sim \mathcal{N}(\theta, 1)$.
    Мы знаем, что тогда выборочное среднее $\overline{X}$ является оптимальной оценкой для $\theta$.
    При этом $\overline{X} \sim \mathcal{N}(\theta,\ 1/n)$. Выпишем функцию распределения $\overline{X}$:
    \[
        \Pr(\overline{X} < z) = \Phi(\sqrt{n}(z - \theta)),
    \]
где $\Phi$ --- функция распределения стандартного нормального закона.
    Реализуем рассмотренный метод, записав уравнения в следующем виде:
    \[
        \Phi(\sqrt{n}(\overline{X} - \theta)) = \epsilon, \qquad \Phi(\sqrt{n}(\overline{X} - \theta)) = 1 - \epsilon.
    \]
Взяв обратную функцию от обеих частей первого уравнения, получаем, что $\sqrt{n}(\overline{X} - \theta) = \Phi^{-1}(\epsilon)$ --- квантиль порядка $\epsilon$.
    Отсюда находим решение (для второго уравнения аналогично):
    \[
        \theta_1^* = \overline{X} - \frac{\Phi^{-1}(\epsilon)}{\sqrt{n}}, \qquad \theta_2^* = \overline{X} - \frac{\Phi^{-1}(1-\epsilon)}{\sqrt{n}} = \overline{X} + \frac{\Phi^{-1}(\epsilon)}{\sqrt{n}},
    \]
так как $\Phi^{-1}(1-\epsilon) = -\Phi^{-1}(\epsilon)$ в силу симметрии данных квантилей относительно нуля.
    Получили интервал
    \[
        \left(\overline{X} - \frac{\Phi^{-1}(\epsilon)}{\sqrt{n}},\ \overline{X} + \frac{\Phi^{-1}(\epsilon)}{\sqrt{n}}\right)
    \]
длины $2\Phi^{-1}(\epsilon) / \sqrt{n}$.
    Заметим, что хотя концы интервала являются случайными величинами, длина интервала не случайна.
    Допустим, можно брать выборку любого объема (то есть $n$ может быть сколь угодно большим).
    С одной стороны можно задать коэффициент надежности (а соответственно и $\epsilon$), а с другой --- длину интервала.
    Тогда по этим параметрам можно однозначно определить необходимый объем выборки.
    Обратное также верно: по фиксированному объему выборки и одному из параметров определяется третий.
    Мы получили, что в данном случае имеет место связь трех характеристик: объем выборки, надежность, длина доверительного интервала.
\end{example}

\subsection{Центральные статистики}
Рассмотрим еще один метод построения доверительных интервалов.
Напомним, что статистика --- это измеримая функция от выборки.
Теперь мы разрешим функции быть зависимой не только от выборки, но и от неизвестного параметра.
\begin{definition}
    \emph{Центральной статистикой} называется измеримая функция $T(Y, \theta)$ от выборки $Y$ и параметра $\theta$ такая, что выполнены следующие условия:
    \begin{enumerate}
        \item распределение $T$ не зависит от $\theta$,
        \item $T$ как функция от $\theta$, является монотонной.
    \end{enumerate}
\end{definition}

Сразу возникает вопрос: существует ли такая функция? Рассмотрим тот же пример.

\begin{example}
    Пусть снова $Y$ --- выборка из распределения $\mathcal{N}(\theta, 1)$.
    Выборочное среднее имеет распределение $\overline{X} \sim \mathcal{N}(\theta,\ 1/n)$, а функция $T(Y, \theta) = \sqrt{N}(\overline{X} - \theta)$ имеет стандартное нормальное распределение.
    Но тогда $T$ и будет центральной статистикой, поскольку оба условия очевидным образом выполнены.
\end{example}

Рассмотрим еще один менее тривиальный пример.
Но для начала вспомним, что такое хи-квадрат распределение.

\begin{definition}
    Случайная величиная $\chi_k^2$ имеет \emph{хи-квадрат} распределение с $k$ степенями свободы, если ее плотность имеет вид
    \[
        f(z) = \frac{2^{-k/2} z^{k/2 - 1} e^{-z/2}}{\Gamma(k/2)}
    \]
при $z > 0$ и 0 иначе.
\end{definition}

\begin{remark}
    $\chi_k^2 \eqdist Z_1^2 + \ldots + Z_k^2$, где $Z_1, \ldots, Z_k$ --- независимые стандартные нормальные случайные величины.
\end{remark}

\begin{example}
    Пусть $Y$ --- выборка из распределения $\mathcal{N}(1, \theta^2)$.
    Тогда, если мы отнормируем каждую случайную величину из выборки, то получим выборку из стандартного нормального распределения.
    Получаем следующее:
    \[
        T(Y, \theta) = \sum_{i=1}^n \frac{(X_i - 1)^2}{\theta^2} \eqdist \chi_n^2.
    \]
    Снова имеем функцию, распределение которой не зависит от $\theta$ и монотонную при фиксированной выборке.
    Тогда $T$ --- центральная статистика.
\end{example}

Рассмотрим теперь сам метод, связанный с центральными статистиками.
Так как распределение $T(Y, \theta)$ не зависит от $\theta$, то по заданному $\gamma$ (коэффициенту надежности), мы сможем найти такие $v_1, v_2$, что выполнено равенство
\[
    \Pr(v_1 < T(Y, \theta) < v_2) = \gamma.    
\]
Для дискретного случая необходимо искать значения для неравенства <<больше либо равно>>.
Сразу возникает проблема, связанная с тем, что $v_1, v_2$ могут быть определены неоднозначно.
Пусть все же нашлись некоторые значения для $v_1, v_2$.
Далее решим уравнения
\[
    T(Y, \theta) = v_1, \qquad T(Y, \theta) = v_2
\]
относительно $\theta$ (аналогично методу точечных оценок).
Пусть $\theta_1^*, \theta_2^*$ --- решения первого и второго уравнений соответственно.
Если $T$ строго возрастающая, то из данных уравнений получаем доверительный интервал $(\theta_1^*, \theta_2^*)$ такой, что
\[
    \Pr(\theta_1^* < \theta < \theta_2^*) = \gamma.
\]
При убывании функции $T$ концы интервала поменяются местами.
Вернемся к примеру с нормальным распределением со средним 1 и неизвестной дисперсией.
\begin{example}
    Пусть $Y$ --- выборка из $\mathcal{N}(1, \theta^2)$, $T(Y, \theta)$ --- найденная центральная статистика.
    Тогда $T \sim \chi_n^2$ и $T$ убывает по $\theta$.
    Пусть $n \geqslant 3$. Тогда значение плотности $T$  в нуле равно нулю и плотность сначала растет, после чего с некоторого места начинает экспоненциально убывать.
    Нам необходимо найти такие $v_1, v_2$, чтобы площадь под кривой между $v_1$ и $v_2$ была равна $\gamma$.
    Ясно, что в данном случае таких значений будет сколь угодно много.
    Возьмем какие-нибудь из возможных значений.
    Решая уравнения $T(Y, \theta) = v_1$ и $T(Y, \theta) = v_2$ и беря во внимания убывание функции $T$ по $\theta$, получаем, что
    \[
        \Pr\left( \frac{1}{v_2} \sum_{i=1}^n (X_i-1)^2 < \theta^2 < \frac{1}{v_1} \sum_{i=1}^n (X_i - 1)^2 \right) = \gamma.
    \]
    В этом случае длина интервала равна
    \[
        \left(\frac1{v_1} - \frac1{v_2}\right) \sum_{i=1}^n(X_i-1)^2
    \]
и является неограниченной случайной величиной.
    Если мы хотим минимизировать длину интервала (в смысле среднего значения), то необходимо искать условный минимум данной функции при ограничениях на $v_1$ и $v_2$ (площадь под кривой плотности должна быть равна $\gamma$) с помощью неопределенных коэффициентов методом Лагранжа.
    Также можно строить \emph{центральный доверительный интервал}, то есть такой, что площади обоих хвостов (вне отрезка с концами $v_1, v_2$) под кривой плотности равны $1 - \gamma / 2$.
\end{example}
