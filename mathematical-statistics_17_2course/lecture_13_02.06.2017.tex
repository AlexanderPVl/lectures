\section{Лекция 13 от 02.06.2017}
\subsection{Критерий хи-квдарат Пирсона}
На практике часто используется так называемый \emph{критерий хи-квадрат 
Пирсона}. Для начала покажем его на примере, после чего сформулируем как 
положено.

Пусть есть выборка \((X_{1}, \dots, X_{n})\) из \(\mathcal{L}(X)\), где \(X\) 
принимает значения \(a_{1}, \dots, a_{k}\) с соответствующими вероятностями 
\(p_{1}, \dots, p_{k}\). Далее, введём какие-то числа \(p_{10}, \dots, p_{k0}\) 
и построим две гипотезы:\footnote{Понятно, что только в одном значении 
отличаться не может, так как сумма \(p_{i}\) равна единице.}
\begin{gather*}
	H_{0} : p_{i} = p_{i0} \text{ для всех } i \\
	H_{1} : p_{i} \neq p_{i0} \text{ хотя бы для двух }i
\end{gather*}

Далее мы будем использовать несколько другую формулировку альтернативы:
\[
	H_{1} : p_{i} = p_{i1} \text{ для всех } i \text{ и } p_{i0} \neq p_{i1}  
	\text{ хотя бы для двух }i
\]

Рассмотрим какую-то реализацию выборки \((x_{1}, \dots, x_{n})\). Понятно, что 
её можно сопоставить набору чисел \(\nu_{1}, \dots, \nu_{k}\), где 
\(\nu_{i}\)~--- это количество чисел из реализации выборки, равных \(a_{i}\). 
Отсюда сразу получается, что \(\nu_{1} + \dots + \nu_{k} = n\). Если же 
смотреть на выборку, то \(\nu_{i}\) являются случайной величиной, равной
\[
	\nu_{i} = \sum_{j = 1}^{n} \I\{X_{j} = a_{i}\}.
\]

Предположим, что выполнена гипотеза \(H_{0}\). Тогда несложно понять, что 
\(\nu_{i} \sim \mathrm{Bin}(n, p_{i0})\), так как \(\I\{X_{j} = a_{i}\}\) будет 
равно единице с вероятностью \(p_{i0}\). Из этого следует, что \(\E{\nu_{i}} = 
np_{i0}\), а \(\D{\nu_{i}} = np_{i0}(1 - p_{i0})\). Теперь составим следующую 
статистику:
\[
	\hat{\chi}^{2}_{n} = \sum_{i = 1}^{k}\frac{(\nu_{i} - np_{i0})^{2}}{np_{i0}}
\]

Рассмотрим следующий пример. Пусть \(X\) принимает значения \(a_{1}, a_{2}, 
a_{3}, a_{4}\), и, в предположении нулевой гипотезы, все значения принимается с 
вероятностью \(1/4\). Предположим, что реализация выборки даёт следующие 
значения для \(\nu_{i}\): 20, 30, 30, 20. Кажется ли такая ситуация критичной 
для гипотезы \(H_{0}\)? Не особо. Заметим, что в данном случае 
\(\hat{\chi}^{2}_{4} = 0,002\).

Теперь изменим вероятности: пусть \(a_{1}, a_{2}, a_{3}, a_{4}\) принимаются с 
вероятностями \(1/100\), \(49/100\), \(49/100\), \(1/100\). И после наблюдений 
мы получили следующие количества вхождений: 6, 44, 44, 6. Интуитивно такой 
исход кажется весьма маловероятным. Если посчитать нашу статистику в этом 
случае, то получится \(0,005\). Как видите, она увеличилась.

Интуитивный вывод следующий: чем меньше \(\hat{\chi}^{2}_{n}\), тем больше 
данные соответствуют гипотезе \(H_{0}\). В чём основная проблема? Мы знаем, что 
в задаче проверке гипотез мы можем допускать ошибки (первого и второго рода). 
Как мы говорили, мы фиксируем вероятность ошибки первого рода и минимизируем 
вероятность ошибки второго рода.

Теперь сформулируем сам критерий:

\begin{theorem}[Критерий хи-квадрат Пирсона]
	Если верна гипотеза \(H_{0}\), то \(\hat{\chi}^{2}_{n} \dto \chi^{2}_{k - 
	1}\).
\end{theorem}
\begin{proof}
	Докажем частный случай: \(k = 2\). В данном случае \(\nu_{2} = n - 
	\nu_{1}\), а \(p_{20} = 1 - p_{10}\). Тогда статистика равна
	\begin{align*}
		\hat{\chi}^{2}_{n} = \frac{(\nu_{1} - np_{10})^{2}}{np_{10}} + \frac{(n 
		- \nu_{1} - n + np_{10})^{2}}{n(1 - p_{10})} = \frac{(\nu_{1} - 
		np_{10})^{2}}{np_{10}(1 - p_{10})} = \left(\frac{\nu_{1} - 
		np_{10}}{\sqrt{np_{10}(1 - p_{10})}}\right)^{2}
	\end{align*}
	
	Теперь заметим, что выражение в скобках равно
	\[
		\frac{\nu_{1} - \E{\nu_{1}}}{\sqrt{\D{\nu_{1}}}}
	\]
	
	Согласно центральной предельной теореме это стремится к \(\mathcal{N}(0, 
	1)\) по распределению. Тогда \(\hat{\chi}^{2}_{n}\) стремится по 
	распределению к \((\mathcal{N}(0, 1))^{2}\), что равно \(\chi^{2}_{1}\).
\end{proof}

Для критерия Пирсона вероятность ошибки первого рода обычно вводится сделующим 
образом:
\[
	\alpha = \Pr{\hat{\chi}^{2}_{n} > \hat{\chi}^{2}_{\text{крит}} \given H_{0}}
\]

Пользуясь критерием, получаем, что её можно записать в таком виде:
\[
	\alpha \approx \Pr{\hat{\chi}^{2}_{n} > c_{\text{крит}}}, \text{ где } 
	c_{\text{крит}} = \text{квантиль }\chi^{2}_{k - 1}\text{ порядка } 1 - 
	\alpha.
\]

У этого критерия есть одно полезное свойство:
\begin{theorem}
	Критерий хи-квадрат Пирсона является состоятельным, то есть мощность 
	критерия стремится к единице при \(n \to \infty\) при условии, что гипотеза 
	\(H_{1}\) простая.
\end{theorem}
\begin{proof}
	По сути, нужно показать, что вероятность ошибки второго рода стремится к 
	нулю. Распишем её:
	\[
		\beta = \Pr{H_{0} \given H_{1}} = \Pr{\hat{\chi}^{2}_{n} \leq 
		c_{\text{крит}} \given H_{1}}
	\]
	
	Подставим определение \(\hat{\chi}^{2}_{n}\), после чего внутри квадрата 
	добавим и вычтем \(np_{i1}\):
	\begin{align*}
		\hat{\chi}^{2}_{n} = \sum_{i = 1}^{k}\frac{(\nu_{i} - 
		np_{i0})^{2}}{np_{i0}} = \sum_{i = 1}^{k}\frac{(\nu_{i} - 
		np_{i1})^{2}}{np_{i0}} + 2\sum_{i = 1}^{k}\frac{(\nu_{i} - 
		np_{i0})(p_{i1} - p_{i0})}{np_{i0}} + n\sum_{i = 1}^{k}\frac{(p_{i1} - 
		p_{i0})^{2}}{p_{i0}}
	\end{align*}
	
	Обозначим первую, вторую и третью суммы за \(T_{1}\), \(T_{2}\) и \(c\) 
	соответсвенно. Теперь заметим, что \(T_{1} \geq 0\), а \(c\)~--- константа. 
	Тогда \(\beta\) равна
	\[
		\Pr{2T_{2} \leq c_{\text{крит}} - nc - T_{1} \given H_{1}}
	\]
	Введя константу \(2c_{1} = c - c_{\text{крит}}/n\), получаем по неравенству 
	Маркова, что
	\[
		\beta \leq \Pr{T_{2} \leq -nc_{1}} \leq \Pr{|T_{2}| \geq 
		nc_{1}} \leq \frac{\E{T_{2}^{2}}}{n^{2}c_{1}^{2}}
	\]
	
	Теперь вспомним неравенство Гёльдера:
	\[
		\sum_{i = 1}^{k} |a_{i}b_{i}| \leq \left(\sum_{i = 1}^{k} 
		a_{i}^{2}\right)^{1/2}\left(\sum_{i = 1}^{k} b_{i}^{2}\right)^{1/2}
	\]
	
	Далее, положим в нём \(a_{i} = \nu_{i} - np_{i1}\), \(b = (p_{i1} - 
	p_{i0})/p_{i0}\). Тогда
	\[
		|T_{2}|^{2} \leq \left(\sum_{i = 1}^{n} (\nu_{i} - 
		np_{i1})^{2}\right)\left(\sum_{i = 1}^{n} \left(\frac{p_{i1} - 
		p_{i0}}{p_{i0}}\right)^{2}\right)
	\]
	
	Взяв матожидание с обеих сторон, получаем, что
	\[
		\E{T_{2}^{2}} \leq const \cdot \sum_{i = 1}^{n} \underbrace{\E{(\nu_{i} 
		- np_{i1})^{2}}}_{= \D{\nu_{i}}} \leq const \cdot n
	\]
	
	Тем самым получаем, что
	\[
		\beta \leq \frac{const}{n} \xrightarrow[n \to \infty]{} 0. \qedhere
	\]
\end{proof}

Теперь введём два обобщения без каких-либо доказательств.
\begin{enumerate}
	\item Если \(p_{i0} = p_{i0}(\theta)\), где \(\theta \in \Theta \subseteq 
	\R^{d}\), то для \(\theta^{*}\), равной оценке максимального правдоподобия 
	для \(\theta\) 
	выполнено
	\[
		\hat{\chi}^{2}_{n}(\theta^{*}) \dto \chi^{2}_{k - 1 - d}
	\]
	
	\item Пусть выделено \(k\) чисел \(a_{1}, a_{2}, \dots, a_{k}\) и случайная 
	величина \(X\) принимает значения из соответствующих интервалов с 
	вероятностями \(p_{1}, \dots, p_{k}\). Далее, берём выборку из такого 
	распредения и строим две гипотезы:
	\begin{gather*}
		H_{0} : \mathcal{L}(X) = \mathcal{N}(0, 1) \\
		\tilde{H_{0}} : p_{i} \neq p_{i0}, \text{ где } p_{i0} = 
		\int\limits_{a_{i - 1}}^{a_{i}} \phi(z)\diff z
	\end{gather*}
	
	Тогда
	\[
		\frac{\nu_{i} - np_{i0}}{\sqrt{np_{i0}(1 - p_{i0})}} \dto 
		\mathcal{N}(0, 1)
	\]
\end{enumerate}

Когда стоит применять критерий хи-квадрат? Обычно рекомендуемые условия: \(n 
\geq 50\), \(\min \nu_{i} \geq 5\).