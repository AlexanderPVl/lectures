\section{Лекция 6 от 07.04.2017}
\subsection{Полные статистики}

Мы продолжаем рассматривать выборки $(X_1, \ldots, X_n)$ из некоторого параметрического распределения $\mathcal{L}(X)$. При этом параметр может быть многомерным. Наша задача --- по выборке определить, какое из возможных распределений наилучшим образом (в каком-либо смысле) описывает наши наблюдения. Мы уже изучили некоторые свойства точечных оценок: несмещенность, состоятельность, асимптотическую нормальность, а также оптимальность и эффективность (в рамках некоторых ограничений). Также мы ввели определение достаточных статистик --- тех статистик, в которых находится вся информация о неизвестном параметре $\theta$. Перейдем к изучению полных статистик.
\begin{definition}
    Статистика $T = T(X_1, \ldots, X_n)$ называется \emph{полной}, если для всякой измеримой функции $\phi(T)$ из того, что $\E_\theta \phi(T) = 0$ при всех значениях $\theta \in \Theta$, следует, что $\Pr_\theta \big( \phi(T) = 0\big) = 1$ для всех $\theta \in \Theta$.
\end{definition}

Вспомним пример с равномерным распределением на отрезке $[0,\ \theta]$. 
\begin{example}
    Мы выяснили, что оценка $X_{(n)}$ является достаточной статистикой. Также мы знаем, что функция распределения $X_{(n)}$ и ее плотность имеют следующий вид:
    \[
        \Pr(X_{(n)} \leqslant z) = F_{X_1}^n(z) = \frac{z^n}{\theta^n}, \qquad p_{X_{(n)}}(z) = \frac{nz^{n-1}}{\theta^n}.
    \]
Рассмотрим произвольную функцию $\phi(X_{(n)})$. Пусть $\E_\theta\phi(X_{(n)}) = 0$ при всех значениях параметра $\theta > 0$. Выпишем формулу математического ожидания:
    \[
        \int\limits_0^\theta \phi(z)\ \frac{nz^{n-1}}{\theta^n}\ dz = 0 \quad \Leftrightarrow \quad \int\limits_0^\theta \phi(z)z^{n-1}\ dz = 0.
    \]
Последнее равенство выполнено для любого $\theta$. Из свойств интеграла Лебега можно заключить, что $\phi(z) = 0$ почти наверное. По определению $X_{(n)}$ является полной статистикой.
\end{example}

Теперь докажем простую теорему, раскрывающую смысл определения полной статистики.

\begin{theorem}
    Если существует полная достаточная статистика, то любая фунцкия от нее является оптимальной оценкой своего математического ожидания (если оно существует).
\end{theorem}

\begin{proof}
    Пусть $T$ --- полная достаточная статистика, а $H(T)$ --- произвольная функция, у которой существует математическое ожидание $\E_\theta H(T) = \tau(\theta)$. Предположим, что $H_1$ есть оптимальная оценка для $\tau(\theta)$, то есть $\E_\theta H_1 = \tau(\theta)$. Но $T$ --- достаточная статистика, поэтому в силу теоремы Рао-Блэкуэлла-Колмогорова $H_1$ есть функция от $T$. Имеем следующее:
    \[
        \E{H(T) - H_1(T)} = 0 \quad \text{ почти наверное}.
    \]
Так как под знаком математического ожидания стоит функция от $T$, то из полноты $T$ получаем $H(T) = H_1(T)$ почти наверное.
\end{proof}

\begin{example}
    Снова обратимся к равномерному распределению на $[0,\ \theta]$. Посчитаем математическое ожидание $X_{(n)}$:
    \[
        \E X_{(n)} = \int\limits_0^\theta z\ \frac{nz^{n-1}}{\theta^n} = \frac{n}{n+1}\ \theta.
    \]
Рассмотрим статистику $X_{(n)}^* = (n+1)X_{(n)} / n$. Это функция от полной достаточной статистики, а значит, оптимальная оценка своего математического ожидания. Несложно заметить, что последнее в точности равно $\theta$. Получили оптимальную оценку неизвестного параметра.
\end{example}

Мы научились находить оптимальные оценки, что крайне полезно. Однако теперь появляется следующая проблема --- доказательство полноты некоторой оценки. Решение данной проблемы зачастую весьма нетривиально. Но для некоторых распределений, например, экспоненциальных, существуют достаточные условия полноты.

Напомним, что для одномерного параметра $\theta$ и выборки объема один экспоненциальное семейство имеет место, если
    \[
        p_1(x, \theta) = \exp\Big[A(\theta)B(x) + C(\theta) + D(x)\Big].
    \]
Пусть теперь параметр $\theta$ многомерный, то есть $\theta = (\theta_1, \ldots, \theta_k)$. В этом случае экспоненциальное семейство обобщается следующим образом:
    \[
        p_1(x, \theta) = \exp\Big[\sum\limits_{i=1}^kA_i(\theta)B_i(x) + C(\theta) + D(x)\Big].
    \]
Найдем функцию правдоподобия выборки размера $n$:
    \[
        p_n(x_1, \ldots, x_n, \theta) = \exp\Big[\langle A(\theta), S \rangle + nC(\theta) + \sum\limits_{i=1}^nD(x_i)\Big],
    \]
где
    \[
        A(\theta) = \big(A_1(\theta), \ldots, A_k(\theta)\big), \qquad S = (S_1, \ldots, S_k), \qquad S_j = \sum\limits_{i=1}^n B_j(x_i).
    \]

В рамках экспоненциального семейства и вышеописанных обозначений имеем следующую теорему:
\begin{theorem}
    Если функция $A(\theta)$ и множество $\Theta$ таковы, что $A(\theta)$ зачеркивает $k$-мерный параллелепипед (возможно, бесконечный) при изменении $\theta \in \Theta$, то вектор $S$ является полной достаточной статистикой.
\end{theorem}

Доказательство данной теоремы технически сложное, поэтому мы оставим ее без доказательства. Отметим, что на данный момент это единственное известное нам достаточное условие того, что некоторая статистика является полной.

Вернемся к методам построения статистик.

\subsection{Оценка максимального правдоподобия}

Мы уже изучили метод моментов построения точечных оценок и отметили, что это не самый удачный инструмент для нахождения статистик. Рассмотрим теперь \emph{метод максимального правдоподобия}, который используется крайне часто. Чтобы был ясен смысл этих слов, мы начнем с естественного примера.

\begin{example}
    Пусть $x_1, x_2$ --- два наблюдения из $Ber(\theta)$. Предположим, что нам известно следующее: $\Theta = \{0.1,\ 0.8\}$. Какое значение параметра стоит взять в качестве оценки в случае $(x_1,\ x_2) = (1,\ 1)$? Можно посмотреть на вероятность появления такого наблюдения:
    \[
        \Pr\big( (x_1,\ x_2) = (1, 1) \big) = \begin{cases} 0.01, & \theta = 0.1, \\ 0.64, & \theta = 0.8. \end{cases}
    \]
Несмотря на то что вероятность появления данной выборки положительна при обоих значениях параметра $\theta$, она сильно больше во втором случае, поэтому естественно считать лучшей оценкой $\theta = 0.8$. Иными словами, мы пытаемся добиться того, чтобы оценка была максимально правдоподобной.
\end{example}

\begin{definition}
    Оценка $\theta^* = \theta^*(X_1, \ldots, X_n)$ называется \emph{оценкой максимального правдоподобия}, если
    \[
        p_n(y, \theta^*) = \sup_{\theta \in \Theta}\ p_n(t, \theta).
    \]
\end{definition}

Несложно понять, что в последнем примере мы нашли не что иное, как оценку максимального правдоподобия.

Заметим, что вместо максимизации функции правдоподобия $p_n(y, \theta)$ можно максимизировать любую монотонную функцию от нее. Так как значение $p_n(y, \theta)$  всегда положительно, можно максимизировать логарифм функции правдоподобия. Это удобно, поскольку в этом случае логарифм произведения вероятностей (или плотностей) распадается в сумму логарифмов. А дифференцировать сумму функций технически проще, чем их произведение. Поэтому если $\theta = (\theta_1, \ldots, \theta_k)$, функция правдоподобия $p_n(y, \theta)$ дифференцируема в области $\Theta$ и ее максимум достигается внутри $\Theta$, то необходимое условие максимума есть система
    \[
        \dfrac{\partial\ln p_n(y, \theta)}{\partial \theta} = 0, \quad i = 1, 2, \ldots, k,
    \]
которая называется \emph{системой уравнений правдоподобия}.

\begin{point}
    Если существует эффективная оценка скалярного параметра $\theta$, то она является оценкой максимального правдоподобия.
\end{point}

\begin{proof}
    Пусть $T$ --- эффективная оценка параметра $\theta$ и $T(y) = \theta_0$. Из критерия Рао-Крамера следует, что
    \[
        \dlnpn{y} = c(\theta) \big( T(y) - \theta \big).
    \]
Также из неравенства Рао-Крамера имеем
    \[
        \D T = \frac{1}{c(\theta)} \tau'(\theta).
    \]
Но $\tau(\theta) = \theta$, поэтому можно сделать вывод, что $c(\theta) > 0$ на $\Theta$. Осталось заметить, что если параметр $\theta$ проходит через точку $\theta_0$, то производная логарифма функции правдоподобия меняет знак с плюса на минус. Причем $\theta_0$ --- единственная такая точка, поэтому на ней достигается максимум правдоподобия.
\end{proof}

Получается, что если эффективные оценки есть, то это оценки максимального правдоподобия. Уже этого достаточно, чтобы рассматривать метод максимального правдоподобия.
