\section{Лекция 14 от 09.06.2017}
\subsection{Использование критерия хи-квадрат Пирсона в задачах проверки 
независимости и однородности}
\subsubsection{Проверка независимости}
Пусть имеется выборка $(X_1,\ Z_1), \ldots, (X_n,\ Z_n)$ двумерных
векторов из некоторого распределения $\mathcal{L}(X,\ Z)$. Мы рассмотрим
случай, когда случайные величины $X$ и $Z$ дискретны, то есть
\begin{alignat}{2}
    \text{$X$ принимает значения} & \quad a_1 \ldots a_k & \qquad \text{$Z$
    принимает значения} & \quad b_1 \ldots b_m \\
    \text{с вероятностями} & \quad p_1 \ldots p_k, & \qquad \text{с
    вероятностями} & \quad q_1 \ldots q_m.
\end{alignat}

Рассмотрим гипотезу $H_0$, заключающуюся в том, что $X$ и $Z$ независимы.
Конкурирующая гипотеза формулируется просто $H_1$: иначе.

\begin{example}
    Пусть $X$ --- это заработная плата. Эта величина и так дискретна, но
    можно ограничиться даже тремя значениями: малая, средняя и высокая
    заработная плата. А $Z$ --- это, допустим, уровень образования, то
    есть выпускник школы, бакалавриата, магистратуры и так далее. Тогда
    проверка гипотезы $H_0$ покажет, зависит ли заработная плата от
    уровня образования.
\end{example}

Рассмотрим случайные величины $(\nu)_{ij},\ i = 1..k,\ j = 1..m$, где
$\nu_{ij}$ есть количество элементов $(a_i,\ b_j)$ в выборке\footnote{
Если из этих величин составить таблицу, получится так называемая
матрица сопряженности.}. Очевидное наблюдение: $\sum \nu_{ij} = n$.
Можно выписать статистику хи-квадрат:
\[
    \hat{\chi}^2 = \sum_{i=1}^k \sum_{j=1}^m \frac{(\nu_{ij} -
    np_{ij})^2}{np_{ij}} \quad \dto \quad \chi^2_{km-1}.
    \]
Проблема в следующем: нам не известны значения $p_{ij}$. Попробуем
заменить их на оценки. Заметим, что гипотеза $H_0$ заключается в
следующем:
\[
    \underbrace{\Pr(X = a_i,\ Z = b_j)}_{p_{ij}} = \underbrace{\Pr(
    X = a_i)}_{p_i} \underbrace{\Pr(Z = b_j)}_{q_j}
    \]
для всех $i,\ j$. Для чисел $p_i,\ q_j$ можно рассмотреть оценки
\[
    p_i^* = \frac1n \sum_{j=1}^m \nu_{ij} = \frac{\nu_{i\circ}}n,
    \qquad q_j^* = \frac1n \sum_{i=1}^k \nu_{ij} = \frac{ 
    \nu_{\circ j}}n.
    \]
По сути, $p_i^*,\ q_j^*$ --- это частоты появления $a_i$ и $b_j$
в выборке соответственно. Можно показать, что эти оценки суть
оценки максимального правдоподобия. Тогда в предположении гипотезы
$H_0$ статистику хи-квадрат можно переписать так:
\[
    \hat{\chi}^2 = \sum_{i=1}^k \sum_{j=1}^m \frac{(\nu_{ij} -
    np_i^*q_j^*)^2}{np_i^*q_j^*} \quad \dto \quad \chi^2_{km-1-d},
    \]
где $d$ --- число добавленных оценок максимального правдоподобия.
С одной стороны, мы добавили $k + m$ статистик. Однако, беря
во внимание
\[
    \sum_{i=1}^k p_i^* = 1 \qquad \text{и} \qquad
    \sum_{j=1}^m q_j^* = 1,
    \]
получаем, что последние статистики выражаются через предыдущие.
Поэтому $d = (k - 1) + (m - 1)$ и статистика сходится к
распределению хи-квадрат с $(k-1)(m-1)$ степенями свободы.

Получаем следующий критерий: $H_0$ отвергается, если
$\hat{\chi}^2 > \chi^2_{\text{крит}}$, где $\chi^2_{\text{крит}}$
определяется из соотношения
\[
    \Pr(\chi^2_{(k-1)(m-1)} > \chi^2_{\text{крит}}\ |\ H_0) = \alpha.
    \]

\subsubsection{Проверка однородности}

Пусть имеется $m$ каких-то выборок:
\begin{alignat}{4}
    X_1^{(1)}, \ldots, X_{n_1}^{(1)} & \qquad & \text{из} & \quad \mathcal{L}(X_1),\\
    & \vdots & &\\
    X_1^{(m)}, \ldots, X_{n_m}^{(m)} & \qquad & \text{из} & \quad \mathcal{L}(X_m),
\end{alignat}

Снова предположим, что все случайные величины дискретны и имеют общий
набор значений $a_1, \ldots, a_k$. Обозначим $p_{ij} = \Pr(X_i = a_j)$.
Гипотеза $H_0$ заключается в том, что $\mathcal{L}(X_1) = \ldots =
\mathcal{L}(X_m)$, то есть все распределения суть одно и то же распределение.
Снова $H_1$ --- это отрицание $H_0$.

Решать задачу будем аналогично. Снова рассмотрим величины $(\nu)_{ij}$,
$i = 1..m,\ j = 1..k$, но сейчас $\nu_{ij}$ отвечает за количество чисел
$a_j$ в $i$-й выборке. Выпишем статистику хи-квадрат по каждой выборке,
после чего сложим:
\[
    \hat{\chi}^2 = \sum_{i=1}^m \sum_{j=1}^k \frac{(\nu_{ij} -
    n_ip_{ij})^2}{n_ip_{ij}} \quad \dto \quad \underbrace{\chi^2_{k-1}
    + \ldots + \chi^2_{k-1}}_{m} \eqdist \chi^2_{m(k-1)}.
    \]

И снова значения $p_{ij}$ нам неизвестны. В данном случае гипотеза $H_0$
состоит в том, что для фиксированного $j$ выполнено $p_{1j} = \ldots =
p_{mj} = p_j$. Для $p_j$ рассмотрим следующую оценку (она также является
оценкой максимального правдоподобия):
\[
    p_j^* = \frac1n \sum_{i=1}^m \nu_{ij} = \frac{\nu_{\circ j}}n, \qquad
    \text{причем снова} \quad \sum_{j=1}^k p_j^* = 1.
    \]
Отсюда вытекает, что мы добавляем $k-1$ оценку. Тогда критерий хи-квадрат
переписывается следующим образом:
\[
    \hat{\chi}^2 = \sum_{i=1}^m \sum_{j=1}^k \frac{(\nu_{ij} -
    n_ip_j^*)^2}{n_ip_j^*} \quad \dto \quad \chi^2_{(m-1)(k-1)}.
    \]
Получаем критерий проверки однородности, в точности аналогичный проверке
на независимость.

\begin{example}
    Часто на практике приходится иметь дело с двумя выборками: хочется
    знать, имеют ли они одно распределение. В этом случае хи-квадрат
    статистика принимает вид
    \[
        \hat{\chi}^2 = n_1n_2 \sum_{i=1}^k \frac1{\nu_{1i} + \nu_{2i}}
        \left( \frac{\nu_{1i}}{n_1} - \frac{\nu_{2i}}{n_2} \right)^2,
        \]
    а сам критерий остается тем же.
\end{example}

\subsection{Проверка гипотез и доверительные интервалы}
Пусть имеется выборка $X_1, \ldots, X_n$ из $\mathcal{N}(\theta,\ 1)$.
Мы знаем, что хороший доверительный интервал для $\theta$ при заданном
коэффициенте надежности $\gamma$ будет иметь вид $(\overline{X} -
\epsilon(\gamma), \overline{X} + \epsilon(\gamma))$. Однако если бы
проверялась гипотеза $H_0$ о том, что среднее равно $\theta$ против
обратной альтернативной гипотезы $H_1$, то $S$-критерий выглядел бы
следующим образом: $H_0$ отвергается, если $|\overline{X} - \theta|
> x_{\text{крит}}$, где $x_{\text{крит}}$ определяется из соотношения
\[
    \Pr(|\overline{X} - \theta| > x_{\text{крит}}\ |\ H_0) = \alpha.
    \]
Заметим, что получившиеся объекты --- критическая область и доверительный
интервал --- очень похожи. Это позволяет сделать следующее наблюдение.

Если проверяется гипотеза $H_0:\ \theta = \theta_0$ в задаче с выборкой
$Y = (X_1, \ldots, X_n)$ из параметрического семейства $\mathcal{L}(X) =
\{\Pr_\theta,\ \theta \in \Theta\}$ и используется $S$-критерий, то
для любого $\theta_0$ указывается область $S \in \X$ такая, что при $Y
\in S$ $H_0$ отвергается и $\alpha = \Pr(H_1\ |\ H_0)$. Тогда $\overline{S}$
есть область, где $H_0$ принимается. То есть для любого $\theta \in \Theta$
мы указываем такую область $\overline{S}(\theta) \in \X$, что
\[
    \Pr\big( (X_1, \ldots, X_n) \in \overline{S}(\theta)\ |\ H_0 \big) =
    1 - \alpha.
    \]
С другой стороны, для любого $y \in \X$ определим подмножество $T(y) \subset
\Theta$ так: $T(y) = \{\theta:\ y \in \overline{S}(\theta)\}$. Но тогда
по построению $T(Y)$ и будет доверительным множеством для параметра $\theta$
с уровнем доверия $1 - \alpha$. Таким образом, задача о построении
доверительного интервала двойственна задаче о проверки такой статистической
гипотезы. Более того, часто оказывается, что интервал минимальной длины
дает наиболее мощный критерий и наоборот.

\subsection{Последнее замечание}
Зачастую вводят так называемые \emph{$p$-значения} следующим образом:
\[
    p_{\text{знач}} = \Pr(\hat{\chi}^2 > \hat{\chi}^2_{\text{набл}}\ |\ H_0),
    \]
где $\hat{\chi}^2_{\text{набл}}$ --- наблюдаемое в конкретной выборке
значение статистики хи-квадрат. Пусть у нас есть некоторый критерий с для
гипотезы $H_0$ с уровнем значимости $\alpha$: $H_0$ отвергается, если
$\hat{\chi}^2 > \chi^2_{\text{крит}}$. Как записать этот критерий с помощью
$p$-значений? Очень просто: $H_0$ отвергается, если $p_{\text{знач}} < \alpha$.
Действительно, это очевидным образом следует из
\[
    p < \alpha \quad \Longleftrightarrow \quad \hat{\chi}^2_{\text{набл}}
    > \chi^2_{\text{крит}}.
    \]

